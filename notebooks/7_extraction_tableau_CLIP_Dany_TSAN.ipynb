{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "YUiAWipjbi10",
        "outputId": "7621e4b3-0da5-47e5-ee76-f1a8c61d96e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.6.15)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m67.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m45.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m77.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers torch torchvision"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install psutil"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "wC1HWk2fgRGu",
        "outputId": "3f6727a8-5d3c-43cf-9d2e-8fc9cafcad56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (5.9.5)\n",
            "Collecting PyMuPDF\n",
            "  Downloading pymupdf-1.26.3-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (3.4 kB)\n",
            "Downloading pymupdf-1.26.3-cp39-abi3-manylinux_2_28_x86_64.whl (24.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m78.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyMuPDF\n",
            "Successfully installed PyMuPDF-1.26.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/openai/CLIP.git"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ggOlmLlygZZg",
        "outputId": "bebd7116-af7e-48a0-d976-c055574837a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-jhj5w5h2\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-jhj5w5h2\n",
            "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ftfy (from clip==1.0)\n",
            "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (24.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (4.67.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (0.21.0+cu124)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy->clip==1.0) (0.2.13)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->clip==1.0) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision->clip==1.0) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->clip==1.0) (11.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->clip==1.0) (3.0.2)\n",
            "Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: clip\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369490 sha256=60b56d36906f0f525b2c56eddee4275bb2439252d65b987beb11469b059c148a\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-7a1rm11f/wheels/3f/7c/a4/9b490845988bf7a4db33674d52f709f088f64392063872eb9a\n",
            "Successfully built clip\n",
            "Installing collected packages: ftfy, clip\n",
            "Successfully installed clip-1.0 ftfy-6.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Réinstaller à chaque lancement du NB\n",
        "!pip install PyMuPDF"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pmhcgty6gWC9",
        "outputId": "996f36a2-8452-46bf-c2f4-56d01626d224"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: PyMuPDF in /usr/local/lib/python3.11/dist-packages (1.26.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "from transformers import AutoProcessor, AutoModel\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "\n",
        "from PIL import Image\n",
        "import fitz # relancer !pip install PyMuPDF\n",
        "import io\n",
        "\n",
        "import time\n",
        "import psutil\n",
        "import tracemalloc\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, hamming_loss, jaccard_score\n",
        "\n",
        "import os\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "V_ljBdR0gd6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6bMHljXgyRb",
        "outputId": "34f33572-a76b-47c3-c4d3-147b5b6398d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    print(f'📊 GPU: {torch.cuda.get_device_name(0)}')\n",
        "    print(f'💾 VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OYCe-LoehM2R",
        "outputId": "fd0e23d1-3187-4fa7-c427-1d1b68c00849"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📊 GPU: Tesla T4\n",
            "💾 VRAM: 14.7GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SET UP DU MEASURE PERFORMANCE_V2"
      ],
      "metadata": {
        "id": "id62zGA6GNuW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def measure_performance(func, *args):\n",
        "    \"\"\"Mesure temps et mémoire\"\"\"\n",
        "    process = psutil.Process()\n",
        "    ram_before = process.memory_info().rss / 1024 / 1024\n",
        "\n",
        "    start = time.time()\n",
        "    result = func(*args)\n",
        "    end = time.time()\n",
        "\n",
        "    ram_after = process.memory_info().rss / 1024 / 1024\n",
        "\n",
        "    return {\n",
        "        \"result\": result,\n",
        "        \"time_seconds\": end - start,\n",
        "        \"ram_mb\": ram_after - ram_before\n",
        "    }"
      ],
      "metadata": {
        "id": "HEtyshZAguZD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def measure_performance_v2(func, *args):\n",
        "    \"\"\"Mesure le temps, la RAM CPU (delta + pic), et la VRAM GPU (si dispo), v2 amélioré par chatGPT\"\"\"\n",
        "\n",
        "    process = psutil.Process()\n",
        "    ram_before = process.memory_info().rss / 1024 / 1024\n",
        "\n",
        "    # tracking de la mémoire CPU\n",
        "    tracemalloc.start()\n",
        "\n",
        "    # Check si GPU cuda est dispon\n",
        "    cuda_available = torch.cuda.is_available()\n",
        "    if cuda_available:\n",
        "        torch.cuda.reset_peak_memory_stats()\n",
        "\n",
        "    # Calcul du temps\n",
        "    start = time.time()\n",
        "    result = func(*args)\n",
        "    end = time.time()\n",
        "\n",
        "    # Post processing, calcul de la RAM CPU\n",
        "    current, peak = tracemalloc.get_traced_memory()\n",
        "    tracemalloc.stop()\n",
        "    ram_after = process.memory_info().rss / 1024 / 1024\n",
        "\n",
        "    # Post processing, calcul de la RAM GPU\n",
        "    if cuda_available:\n",
        "        vram_peak_mb = torch.cuda.max_memory_allocated() / 1024 / 1024\n",
        "    else:\n",
        "        vram_peak_mb = None\n",
        "\n",
        "    return {\n",
        "        \"result\": result,\n",
        "        \"time_seconds\": end - start,\n",
        "        \"cpu_ram_mb_delta\": ram_after - ram_before,\n",
        "        \"cpu_ram_peak_mb\": peak / 1024 / 1024,\n",
        "        \"gpu_vram_peak_mb\": vram_peak_mb\n",
        "    }"
      ],
      "metadata": {
        "id": "vwHtgulqyQ33"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformation pdf en image dans le bon dossier\n",
        "\n",
        "@Aghiles, j'avais mis directement les PP en pdf dans mes dossiers avant la transformation en image."
      ],
      "metadata": {
        "id": "bByeOOMdhETU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "####################################\n",
        "### Changer les chemins relatifs ###\n",
        "####################################\n",
        "\n",
        "pu_p01_pp01 = \"/content/drive/MyDrive/Document AI - GroupeSOS/AAP/PU_P01_PP01.pdf\"\n",
        "save_dir = \"/content/drive/MyDrive/Document AI - GroupeSOS/Outputs/Save_img_from_pdf\""
      ],
      "metadata": {
        "id": "YZSFs66-ggt4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pdf_to_images_split(pdf_path, save_dir):\n",
        "    \"\"\"Convertit le AAP.pdf en AAP_page_X.jpg\"\"\"\n",
        "\n",
        "    # automatisation de récupération du nom de l'AAP ou du PP\n",
        "    pdf_name = os.path.splitext(os.path.basename(pdf_path))[0]\n",
        "\n",
        "    # Check si le dir existe\n",
        "    pdf_folder = os.path.join(save_dir, f\"{pdf_name}_folder\")\n",
        "    os.makedirs(pdf_folder, exist_ok=True)\n",
        "\n",
        "    print(f\"Save directory: {pdf_folder}\")\n",
        "\n",
        "    # Ouvrir le PDF\n",
        "    pdf_doc = fitz.open(pdf_path)\n",
        "    print(f\"PDF pages: {len(pdf_doc)}\")\n",
        "\n",
        "    images = []\n",
        "\n",
        "    # Convertir chaque page séparément\n",
        "    for page_num in range(len(pdf_doc)):\n",
        "        page = pdf_doc.load_page(page_num)\n",
        "        mat = fitz.Matrix(1.0, 1.0) # change la qualité si besoin\n",
        "        pix = page.get_pixmap(matrix=mat)\n",
        "\n",
        "        # Convertir avec PIL\n",
        "        img_data = pix.tobytes(\"ppm\")\n",
        "        img = Image.open(io.BytesIO(img_data))\n",
        "        images.append(img)\n",
        "\n",
        "        # Sauvegarder chaque page\n",
        "        output_name = os.path.join(pdf_folder, f\"{pdf_name}_page_{page_num + 1}.jpg\")\n",
        "        img.save(output_name, \"JPEG\", quality=95)\n",
        "        print(f\"✅ Page {page_num + 1} saved: {output_name}\")\n",
        "\n",
        "    pdf_doc.close()\n",
        "\n",
        "    # Vérification\n",
        "    saved_files = [f for f in os.listdir(pdf_folder)\n",
        "               if f.startswith(pdf_name) and f.endswith('.jpg') and '_page_' in f]\n",
        "\n",
        "    print(f\"✅ PDF pages: {len(images)}\")\n",
        "    print(f\"✅ JPG files: {len(saved_files)}\")\n",
        "    print(f\"✅ Match: {len(images) == len(saved_files)}\")\n",
        "\n",
        "    return images"
      ],
      "metadata": {
        "id": "OoG4KwKuglWY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SET PU_P01_PP01"
      ],
      "metadata": {
        "id": "JQ6WtcvkHwX9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ground_truth_pu_p01_pp01_multilabel = {\n",
        "    \"p1\": {\"text\": True, \"table\": False, \"schema\":False}\n",
        "    ,\"p2\": {\"text\": True, \"table\": False,\"schema\":False}\n",
        "    ,\"p3\": {\"text\": True, \"table\": True, \"schema\":False}\n",
        "    ,\"p4\": {\"text\": True, \"table\": False, \"schema\":False}\n",
        "    ,\"p5\": {\"text\": True, \"table\": False, \"schema\":False}\n",
        "    ,\"p6\": {\"text\": True, \"table\": False, \"schema\":False}\n",
        "    ,\"p7\": {\"text\": True, \"table\": False, \"schema\":True} # Image, carte, par définition c'est un schéma\n",
        "    ,\"p8\": {\"text\": True, \"table\": True, \"schema\":False}\n",
        "    ,\"p9\": {\"text\": True, \"table\": False,\"schema\":False}\n",
        "    ,\"p10\": {\"text\": True, \"table\": False,\"schema\":False}\n",
        "    ,\"p11\": {\"text\": True, \"table\": False,\"schema\":True} # Image, carte avec légende\n",
        "    ,\"p12\": {\"text\": True, \"table\": True, \"schema\":False}\n",
        "    ,\"p13\": {\"text\": True, \"table\": True, \"schema\":False} # tableau en image\n",
        "    ,\"p14\": {\"text\": True, \"table\": False, \"schema\":False}\n",
        "    ,\"p15\": {\"text\": True, \"table\": False, \"schema\":True} # Particularité, c'est une infographie\n",
        "    ,\"p16\": {\"text\": True, \"table\": False, \"schema\":True} # Image, carte avec légende\n",
        "    ,\"p17\": {\"text\": True, \"table\": False, \"schema\":False}\n",
        "    ,\"p18\": {\"text\": True, \"table\": False, \"schema\":False}\n",
        "    ,\"p19\": {\"text\": True, \"table\": False, \"schema\":False}\n",
        "    ,\"p20\": {\"text\": True, \"table\": False, \"schema\":False}\n",
        "    ,\"p21\": {\"text\": True, \"table\": False, \"schema\":False}\n",
        "    ,\"p22\": {\"text\": True, \"table\": True, \"schema\":False}\n",
        "    ,\"p23\": {\"text\": True, \"table\": True, \"schema\":False}\n",
        "    ,\"p24\": {\"text\": True, \"table\": False, \"schema\":False}\n",
        "    ,\"p25\": {\"text\": True, \"table\": False, \"schema\":False}\n",
        "    ,\"p26\": {\"text\": True, \"table\": False, \"schema\":False}\n",
        "    ,\"p27\": {\"text\": True, \"table\": False, \"schema\":False}\n",
        "    ,\"p28\": {\"text\": True, \"table\": False, \"schema\":False}\n",
        "    ,\"p29\": {\"text\": True, \"table\": False, \"schema\":True} # Image, carte avec légende\n",
        "    ,\"p30\": {\"text\": True, \"table\": True, \"schema\":False}\n",
        "    ,\"p31\": {\"text\": True, \"table\": True, \"schema\":False}\n",
        "    ,\"p32\": {\"text\": True, \"table\": True, \"schema\":False}  # Fin de tableau\n",
        "    ,\"p33\": {\"text\": True, \"table\": False, \"schema\":False}\n",
        "    ,\"p34\": {\"text\": True, \"table\": False, \"schema\":False}\n",
        "    ,\"p35\": {\"text\": True, \"table\": True, \"schema\":False}\n",
        "    ,\"p36\": {\"text\": True, \"table\": True, \"schema\":False}\n",
        "    ,\"p37\": {\"text\": True, \"table\": True, \"schema\":False}\n",
        "    ,\"p38\": {\"text\": True, \"table\": True, \"schema\":False}  # Tableau Excel-Like\n",
        "    ,\"p39\": {\"text\": True, \"table\": False, \"schema\":False}\n",
        "    ,\"p40\": {\"text\": True, \"table\": False, \"schema\":False}\n",
        "    ,\"p41\": {\"text\": True, \"table\": True, \"schema\":False}\n",
        "    ,\"p42\": {\"text\": True, \"table\": True, \"schema\":False}\n",
        "    ,\"p43\": {\"text\": True, \"table\": True, \"schema\":False}\n",
        "    ,\"p44\": {\"text\": True, \"table\": True, \"schema\":False}\n",
        "    ,\"p45\": {\"text\": True, \"table\": True, \"schema\":False}\n",
        "    ,\"p46\": {\"text\": True, \"table\": False, \"schema\":False}\n",
        "    ,\"p47\": {\"text\": True, \"table\": True, \"schema\":False} # tableau sous forme de planning\n",
        "    ,\"p48\": {\"text\": True, \"table\": True, \"schema\":False} # tableau sous forme de planning\n",
        "    ,\"p49\": {\"text\": True, \"table\": True, \"schema\":False} # tableau sous forme de planning\n",
        "}"
      ],
      "metadata": {
        "id": "DT_bAG5whVo5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_pu_p01_pp01 = {\n",
        "    \"p1\":\"/content/drive/MyDrive/Document AI - GroupeSOS/Outputs/Save_img_from_pdf/PU_P01_PP01_folder/PU_P01_PP01_page_1.jpg\"\n",
        "    ,\"p2\":\"/content/drive/MyDrive/Document AI - GroupeSOS/Outputs/Save_img_from_pdf/PU_P01_PP01_folder/PU_P01_PP01_page_2.jpg\"\n",
        "    ,\"p3\":\"/content/drive/MyDrive/Document AI - GroupeSOS/Outputs/Save_img_from_pdf/PU_P01_PP01_folder/PU_P01_PP01_page_3.jpg\"\n",
        "    ,\"p4\":\"/content/drive/MyDrive/Document AI - GroupeSOS/Outputs/Save_img_from_pdf/PU_P01_PP01_folder/PU_P01_PP01_page_4.jpg\"\n",
        "    ,\"p5\":\"/content/drive/MyDrive/Document AI - GroupeSOS/Outputs/Save_img_from_pdf/PU_P01_PP01_folder/PU_P01_PP01_page_5.jpg\"\n",
        "    ,\"p6\":\"/content/drive/MyDrive/Document AI - GroupeSOS/Outputs/Save_img_from_pdf/PU_P01_PP01_folder/PU_P01_PP01_page_6.jpg\"\n",
        "    ,\"p7\":\"/content/drive/MyDrive/Document AI - GroupeSOS/Outputs/Save_img_from_pdf/PU_P01_PP01_folder/PU_P01_PP01_page_7.jpg\"\n",
        "    ,\"p8\":\"/content/drive/MyDrive/Document AI - GroupeSOS/Outputs/Save_img_from_pdf/PU_P01_PP01_folder/PU_P01_PP01_page_8.jpg\"\n",
        "    ,\"p9\":\"/content/drive/MyDrive/Document AI - GroupeSOS/Outputs/Save_img_from_pdf/PU_P01_PP01_folder/PU_P01_PP01_page_9.jpg\"\n",
        "    ,\"p10\":\"/content/drive/MyDrive/Document AI - GroupeSOS/Outputs/Save_img_from_pdf/PU_P01_PP01_folder/PU_P01_PP01_page_10.jpg\"\n",
        "    ,\"p11\":\"/content/drive/MyDrive/Document AI - GroupeSOS/Outputs/Save_img_from_pdf/PU_P01_PP01_folder/PU_P01_PP01_page_11.jpg\"\n",
        "    ,\"p12\":\"/content/drive/MyDrive/Document AI - GroupeSOS/Outputs/Save_img_from_pdf/PU_P01_PP01_folder/PU_P01_PP01_page_12.jpg\"\n",
        "    ,\"p13\":\"/content/drive/MyDrive/Document AI - GroupeSOS/Outputs/Save_img_from_pdf/PU_P01_PP01_folder/PU_P01_PP01_page_13.jpg\"\n",
        "    ,\"p14\":\"/content/drive/MyDrive/Document AI - GroupeSOS/Outputs/Save_img_from_pdf/PU_P01_PP01_folder/PU_P01_PP01_page_14.jpg\"\n",
        "    ,\"p15\":\"/content/drive/MyDrive/Document AI - GroupeSOS/Outputs/Save_img_from_pdf/PU_P01_PP01_folder/PU_P01_PP01_page_15.jpg\"\n",
        "    ,\"p16\":\"/content/drive/MyDrive/Document AI - GroupeSOS/Outputs/Save_img_from_pdf/PU_P01_PP01_folder/PU_P01_PP01_page_16.jpg\"\n",
        "    ,\"p17\":\"/content/drive/MyDrive/Document AI - GroupeSOS/Outputs/Save_img_from_pdf/PU_P01_PP01_folder/PU_P01_PP01_page_17.jpg\"\n",
        "    ,\"p18\":\"/content/drive/MyDrive/Document AI - GroupeSOS/Outputs/Save_img_from_pdf/PU_P01_PP01_folder/PU_P01_PP01_page_18.jpg\"\n",
        "    ,\"p19\":\"/content/drive/MyDrive/Document AI - GroupeSOS/Outputs/Save_img_from_pdf/PU_P01_PP01_folder/PU_P01_PP01_page_19.jpg\"\n",
        "    ,\"p20\":\"/content/drive/MyDrive/Document AI - GroupeSOS/Outputs/Save_img_from_pdf/PU_P01_PP01_folder/PU_P01_PP01_page_20.jpg\"\n",
        "    ,\"p21\":\"/content/drive/MyDrive/Document AI - GroupeSOS/Outputs/Save_img_from_pdf/PU_P01_PP01_folder/PU_P01_PP01_page_21.jpg\"\n",
        "    ,\"p22\":\"/content/drive/MyDrive/Document AI - GroupeSOS/Outputs/Save_img_from_pdf/PU_P01_PP01_folder/PU_P01_PP01_page_22.jpg\"\n",
        "    ,\"p23\":\"/content/drive/MyDrive/Document AI - GroupeSOS/Outputs/Save_img_from_pdf/PU_P01_PP01_folder/PU_P01_PP01_page_23.jpg\"\n",
        "    ,\"p24\":\"/content/drive/MyDrive/Document AI - GroupeSOS/Outputs/Save_img_from_pdf/PU_P01_PP01_folder/PU_P01_PP01_page_24.jpg\"\n",
        "    ,\"p25\":\"/content/drive/MyDrive/Document AI - GroupeSOS/Outputs/Save_img_from_pdf/PU_P01_PP01_folder/PU_P01_PP01_page_25.jpg\"\n",
        "    ,\"p26\":\"/content/drive/MyDrive/Document AI - GroupeSOS/Outputs/Save_img_from_pdf/PU_P01_PP01_folder/PU_P01_PP01_page_26.jpg\"\n",
        "    ,\"p27\":\"/content/drive/MyDrive/Document AI - GroupeSOS/Outputs/Save_img_from_pdf/PU_P01_PP01_folder/PU_P01_PP01_page_27.jpg\"\n",
        "    ,\"p28\":\"/content/drive/MyDrive/Document AI - GroupeSOS/Outputs/Save_img_from_pdf/PU_P01_PP01_folder/PU_P01_PP01_page_28.jpg\"\n",
        "    ,\"p29\":\"/content/drive/MyDrive/Document AI - GroupeSOS/Outputs/Save_img_from_pdf/PU_P01_PP01_folder/PU_P01_PP01_page_29.jpg\"\n",
        "    ,\"p30\":\"/content/drive/MyDrive/Document AI - GroupeSOS/Outputs/Save_img_from_pdf/PU_P01_PP01_folder/PU_P01_PP01_page_30.jpg\"\n",
        "    ,\"p31\":\"/content/drive/MyDrive/Document AI - GroupeSOS/Outputs/Save_img_from_pdf/PU_P01_PP01_folder/PU_P01_PP01_page_31.jpg\"\n",
        "    ,\"p32\":\"/content/drive/MyDrive/Document AI - GroupeSOS/Outputs/Save_img_from_pdf/PU_P01_PP01_folder/PU_P01_PP01_page_32.jpg\"\n",
        "    ,\"p33\":\"/content/drive/MyDrive/Document AI - GroupeSOS/Outputs/Save_img_from_pdf/PU_P01_PP01_folder/PU_P01_PP01_page_33.jpg\"\n",
        "    ,\"p34\":\"/content/drive/MyDrive/Document AI - GroupeSOS/Outputs/Save_img_from_pdf/PU_P01_PP01_folder/PU_P01_PP01_page_34.jpg\"\n",
        "    ,\"p35\":\"/content/drive/MyDrive/Document AI - GroupeSOS/Outputs/Save_img_from_pdf/PU_P01_PP01_folder/PU_P01_PP01_page_35.jpg\"\n",
        "    ,\"p36\":\"/content/drive/MyDrive/Document AI - GroupeSOS/Outputs/Save_img_from_pdf/PU_P01_PP01_folder/PU_P01_PP01_page_36.jpg\"\n",
        "    ,\"p37\":\"/content/drive/MyDrive/Document AI - GroupeSOS/Outputs/Save_img_from_pdf/PU_P01_PP01_folder/PU_P01_PP01_page_37.jpg\"\n",
        "    ,\"p38\":\"/content/drive/MyDrive/Document AI - GroupeSOS/Outputs/Save_img_from_pdf/PU_P01_PP01_folder/PU_P01_PP01_page_38.jpg\"\n",
        "    ,\"p39\":\"/content/drive/MyDrive/Document AI - GroupeSOS/Outputs/Save_img_from_pdf/PU_P01_PP01_folder/PU_P01_PP01_page_39.jpg\"\n",
        "    ,\"p40\":\"/content/drive/MyDrive/Document AI - GroupeSOS/Outputs/Save_img_from_pdf/PU_P01_PP01_folder/PU_P01_PP01_page_40.jpg\"\n",
        "    ,\"p41\":\"/content/drive/MyDrive/Document AI - GroupeSOS/Outputs/Save_img_from_pdf/PU_P01_PP01_folder/PU_P01_PP01_page_41.jpg\"\n",
        "    ,\"p42\":\"/content/drive/MyDrive/Document AI - GroupeSOS/Outputs/Save_img_from_pdf/PU_P01_PP01_folder/PU_P01_PP01_page_42.jpg\"\n",
        "    ,\"p43\":\"/content/drive/MyDrive/Document AI - GroupeSOS/Outputs/Save_img_from_pdf/PU_P01_PP01_folder/PU_P01_PP01_page_43.jpg\"\n",
        "    ,\"p44\":\"/content/drive/MyDrive/Document AI - GroupeSOS/Outputs/Save_img_from_pdf/PU_P01_PP01_folder/PU_P01_PP01_page_44.jpg\"\n",
        "    ,\"p45\":\"/content/drive/MyDrive/Document AI - GroupeSOS/Outputs/Save_img_from_pdf/PU_P01_PP01_folder/PU_P01_PP01_page_45.jpg\"\n",
        "    ,\"p46\":\"/content/drive/MyDrive/Document AI - GroupeSOS/Outputs/Save_img_from_pdf/PU_P01_PP01_folder/PU_P01_PP01_page_46.jpg\"\n",
        "    ,\"p47\":\"/content/drive/MyDrive/Document AI - GroupeSOS/Outputs/Save_img_from_pdf/PU_P01_PP01_folder/PU_P01_PP01_page_47.jpg\"\n",
        "    ,\"p48\":\"/content/drive/MyDrive/Document AI - GroupeSOS/Outputs/Save_img_from_pdf/PU_P01_PP01_folder/PU_P01_PP01_page_48.jpg\"\n",
        "    ,\"p49\":\"/content/drive/MyDrive/Document AI - GroupeSOS/Outputs/Save_img_from_pdf/PU_P01_PP01_folder/PU_P01_PP01_page_49.jpg\"\n",
        "}"
      ],
      "metadata": {
        "id": "lZCDxQsRhYO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chargement modèle"
      ],
      "metadata": {
        "id": "v-Q26dLiiNvc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_clip_patch32():\n",
        "  \"\"\"Clip Patch32\"\"\"\n",
        "  processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "  model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "  return processor, model, \"Clip-Patch32\""
      ],
      "metadata": {
        "id": "c57zuP_qjjRA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processor_clip, model_clip, model_name_clip = load_clip_patch32()\n"
      ],
      "metadata": {
        "id": "LcDrzgscqYco",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0cf6b828-b198-406a-8e27-fb440e7535b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Baseline avec CLIP"
      ],
      "metadata": {
        "id": "ee3a7eVgIunq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_multilabel_clip_baseline(image_path,processor, model, model_name='CLIP32'):\n",
        "  \"\"\"Détection binaire multilabel (text/table) avec CLIP32\"\"\"\n",
        "\n",
        "  image = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "  # prompt baseline spécialisé par classe\n",
        "  prompts = {\n",
        "      \"text\":\"document with printed text and readable content\"\n",
        "      ,\"table\":\"document with structured tables and organized data\"\n",
        "  }\n",
        "\n",
        "  results = {}\n",
        "  device = next(model.parameters()).device\n",
        "  for label, prompt in prompts.items():\n",
        "    # Prompts binaires pour chaque classe\n",
        "    binary_prompts = [prompt, f\"document without {label}\"]\n",
        "\n",
        "    inputs = processor(text=binary_prompts\n",
        "                       ,images=image\n",
        "                       ,return_tensors=\"pt\"\n",
        "                       ,padding=True)\n",
        "    inputs = {key: value.to(device) for key, value in inputs.items()}\n",
        "\n",
        "    with torch.no_grad(): # économie de mémoire\n",
        "      outputs = model(**inputs)\n",
        "      probs = outputs.logits_per_image.softmax(dim=1)\n",
        "\n",
        "    score = float(probs[0][0])\n",
        "    results[label] = score\n",
        "\n",
        "    # Seuil baseline\n",
        "  tresholds = {\"text\": 0.5\n",
        "                 ,\"table\": 0.5}\n",
        "  predictions = {label: score > tresholds[label] for label, score in results.items()}\n",
        "\n",
        "  return {\n",
        "      \"model\": model_name\n",
        "      ,\"predictions\": predictions\n",
        "      ,\"scores\": results\n",
        "      ,\"thresholds\":tresholds\n",
        "  }"
      ],
      "metadata": {
        "id": "sWXjiiK0iNbS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fonction d'évaluation\n",
        "\n",
        "F1_score : https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score\n",
        "> classes déséquilibrées (plus de pages text que table)\n",
        "\n",
        "Hamming Loss : https://scikit-learn.org/stable/modules/generated/sklearn.metrics.hamming_loss.html#sklearn.metrics.hamming_loss\n",
        "> pour mesurer erreurs par label individuellement\n",
        "\n",
        "Jaccard : https://scikit-learn.org/stable/modules/generated/sklearn.metrics.jaccard_score.html#sklearn.metrics.jaccard_score\n",
        "> multilabel (text ET table peuvent coexister)"
      ],
      "metadata": {
        "id": "RZ5cTntc9UX1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_binary_multilabel_model(image_folder, ground_truth, detect_function, processor, model):\n",
        "  \"\"\" Evaluation spécifique pour text/table\"\"\"\n",
        "  y_true_text, y_true_table = [], []\n",
        "  y_pred_text, y_pred_table = [], []\n",
        "  y_true_multi, y_pred_multi = [], []\n",
        "\n",
        "  for page_id, true_labels in ground_truth.items():\n",
        "    image_path = os.path.join(image_folder, f\"PU_P01_PP01_page_{page_id[1:]}.jpg\")\n",
        "\n",
        "    result =  detect_function(image_path, processor, model)\n",
        "    pred_labels = result[\"predictions\"]\n",
        "\n",
        "    y_true_text.append(true_labels[\"text\"])\n",
        "    y_true_table.append(true_labels[\"table\"])\n",
        "    y_pred_text.append(pred_labels[\"text\"])\n",
        "    y_pred_table.append(pred_labels[\"table\"])\n",
        "\n",
        "    y_true_multi.append([true_labels[\"text\"], true_labels[\"table\"]])\n",
        "    y_pred_multi.append([pred_labels[\"text\"], pred_labels[\"table\"]])\n",
        "    ### Verbose ###\n",
        "    # print(f\"Shape y_true_multi: {np.array(y_true_multi).shape}\")\n",
        "    # print(f\"Shape y_pred_multi: {np.array(y_pred_multi).shape}\")\n",
        "\n",
        "    # Affichage avec visuel\n",
        "    text_ok = \"✅\" if true_labels[\"text\"] == pred_labels[\"text\"] else \"❌\"\n",
        "    table_ok = \"✅\" if true_labels[\"table\"] == pred_labels[\"table\"] else \"❌\"\n",
        "    print(f\" {page_id}: {text_ok} {table_ok} {schema_ok} | Text:{result['scores']['text']:.3F} vs Table:{result['scores']['table']:.3f}\")\n",
        "\n",
        "  return {\n",
        "        \"f1_text\": f1_score(y_true_text, y_pred_text)\n",
        "        ,\"f1_table\": f1_score(y_true_table, y_pred_table)\n",
        "        ,\"f1_macro\": f1_score(y_true_multi, y_pred_multi, average='macro')\n",
        "        ,\"hamming_loss\": hamming_loss(y_true_multi, y_pred_multi)\n",
        "        ,\"jaccard_macro\": jaccard_score(y_true_multi, y_pred_multi, average='macro')\n",
        "        ,\"jaccard_micro\": jaccard_score(y_true_multi, y_pred_multi, average='micro')\n",
        "        ,\"jaccard_samples\": jaccard_score(y_true_multi, y_pred_multi, average='samples')\n",
        "        ,\"jaccard_per_class\": jaccard_score(y_true_multi, y_pred_multi, average=None)\n",
        "    }\n",
        "\n"
      ],
      "metadata": {
        "id": "0uB-q3AV9SrT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_binary_multilabel_model_v2(image_folder, ground_truth, detect_function, processor, model):\n",
        "  \"\"\" Evaluation spécifique pour text/table\"\"\"\n",
        "  y_true_text, y_true_table, y_true_schema = [], [], []\n",
        "  y_pred_text, y_pred_table, y_pred_schema = [], [], []\n",
        "  y_true_multi, y_pred_multi = [], []\n",
        "\n",
        "  for page_id, true_labels in ground_truth.items():\n",
        "    image_path = os.path.join(image_folder, f\"PU_P01_PP01_page_{page_id[1:]}.jpg\")\n",
        "\n",
        "    result =  detect_function(image_path, processor, model)\n",
        "    pred_labels = result[\"predictions\"]\n",
        "\n",
        "    y_true_text.append(true_labels[\"text\"])\n",
        "    y_true_table.append(true_labels[\"table\"])\n",
        "    y_pred_text.append(pred_labels[\"text\"])\n",
        "    y_pred_table.append(pred_labels[\"table\"])\n",
        "    y_true_schema.append(true_labels[\"schema\"])\n",
        "    y_pred_schema.append(pred_labels[\"schema\"])\n",
        "\n",
        "    y_true_multi.append([true_labels[\"text\"], true_labels[\"table\"], true_labels[\"schema\"]])\n",
        "    y_pred_multi.append([pred_labels[\"text\"], pred_labels[\"table\"], pred_labels[\"schema\"]])\n",
        "    ### Verbose ###\n",
        "    # print(f\"Shape y_true_multi: {np.array(y_true_multi).shape}\")\n",
        "    # print(f\"Shape y_pred_multi: {np.array(y_pred_multi).shape}\")\n",
        "\n",
        "    # Affichage avec visuel\n",
        "    text_ok = \"✅\" if true_labels[\"text\"] == pred_labels[\"text\"] else \"❌\"\n",
        "    table_ok = \"✅\" if true_labels[\"table\"] == pred_labels[\"table\"] else \"❌\"\n",
        "    schema_ok = \"✅\" if true_labels[\"schema\"] == pred_labels[\"schema\"] else \"❌\"\n",
        "    print(f\" {page_id}: {text_ok} {table_ok} {schema_ok} | Text:{result['scores']['text']:.3F} vs Table:{result['scores']['table']:.3f} vs Schema:{result['scores']['schema']:.3f}\")\n",
        "\n",
        "  return {\n",
        "        \"f1_text\": f1_score(y_true_text, y_pred_text)\n",
        "        ,\"f1_table\": f1_score(y_true_table, y_pred_table)\n",
        "        ,\"f1_schema\": f1_score(y_true_schema, y_pred_schema)\n",
        "        ,\"f1_macro\": f1_score(y_true_multi, y_pred_multi, average='macro')\n",
        "        ,\"hamming_loss\": hamming_loss(y_true_multi, y_pred_multi)\n",
        "        ,\"jaccard_macro\": jaccard_score(y_true_multi, y_pred_multi, average='macro')\n",
        "        ,\"jaccard_micro\": jaccard_score(y_true_multi, y_pred_multi, average='micro')\n",
        "        ,\"jaccard_samples\": jaccard_score(y_true_multi, y_pred_multi, average='samples')\n",
        "        ,\"jaccard_per_class\": jaccard_score(y_true_multi, y_pred_multi, average=None)\n",
        "    }\n"
      ],
      "metadata": {
        "id": "Cf9K4iPKlvlC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Baseline test"
      ],
      "metadata": {
        "id": "YnNMhnjQM7_D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path_folder_test_baseline = \"/content/drive/MyDrive/Document AI - GroupeSOS/Outputs/Save_img_from_pdf/PU_P01_PP01_folder\""
      ],
      "metadata": {
        "id": "Q9ztKRRTvRjI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_clip = evaluate_binary_multilabel_model(\n",
        "    path_folder_test_baseline\n",
        "    ,ground_truth_pu_p01_pp01_multilabel\n",
        "    ,detect_multilabel_clip_baseline\n",
        "    ,processor_clip\n",
        "    ,model_clip\n",
        ")\n",
        "\n",
        "print(f\"\\n📊 Résultats CLIP:\")\n",
        "print(f\"F1 Text: {metrics_clip['f1_text']:.3f}\")\n",
        "print(f\"F1 Table: {metrics_clip['f1_table']:.3f}\")\n",
        "print(f\"F1 Macro: {metrics_clip['f1_macro']:.3f}\")\n",
        "print(f\"Hamming loss: {metrics_clip['hamming_loss']:.3f}\")\n",
        "print(f\"Jaccard: {metrics_clip['jaccard_macro']:.3f} & {metrics_clip['jaccard_micro']:.3f}\")\n",
        "print(f\"Jaccard samples :{metrics_clip['jaccard_samples']:.3f}\")\n",
        "print(f\"Jaccard per class: {metrics_clip['jaccard_per_class']}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pl3P-rq9M9Ka",
        "outputId": "0f700545-1012-4c50-fbcc-f723bf0eb1f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " p1: ✅ ✅ | Text:0.983 vs Table:0.020\n",
            " p2: ✅ ✅ | Text:0.934 vs Table:0.014\n",
            " p3: ✅ ❌ | Text:0.978 vs Table:0.106\n",
            " p4: ✅ ✅ | Text:0.964 vs Table:0.030\n",
            " p5: ✅ ✅ | Text:0.957 vs Table:0.099\n",
            " p6: ✅ ✅ | Text:0.969 vs Table:0.017\n",
            " p7: ✅ ✅ | Text:0.818 vs Table:0.213\n",
            " p8: ✅ ❌ | Text:0.889 vs Table:0.053\n",
            " p9: ✅ ✅ | Text:0.968 vs Table:0.044\n",
            " p10: ✅ ✅ | Text:0.962 vs Table:0.022\n",
            " p11: ✅ ✅ | Text:0.733 vs Table:0.061\n",
            " p12: ✅ ❌ | Text:0.948 vs Table:0.319\n",
            " p13: ✅ ❌ | Text:0.960 vs Table:0.120\n",
            " p14: ✅ ✅ | Text:0.976 vs Table:0.030\n",
            " p15: ✅ ❌ | Text:0.905 vs Table:0.027\n",
            " p16: ✅ ✅ | Text:0.941 vs Table:0.075\n",
            " p17: ✅ ✅ | Text:0.969 vs Table:0.009\n",
            " p18: ✅ ✅ | Text:0.968 vs Table:0.018\n",
            " p19: ✅ ✅ | Text:0.930 vs Table:0.026\n",
            " p20: ✅ ✅ | Text:0.975 vs Table:0.024\n",
            " p21: ✅ ✅ | Text:0.923 vs Table:0.032\n",
            " p22: ✅ ❌ | Text:0.873 vs Table:0.024\n",
            " p23: ✅ ❌ | Text:0.859 vs Table:0.202\n",
            " p24: ✅ ✅ | Text:0.884 vs Table:0.060\n",
            " p25: ✅ ✅ | Text:0.937 vs Table:0.026\n",
            " p26: ✅ ✅ | Text:0.964 vs Table:0.017\n",
            " p27: ✅ ✅ | Text:0.942 vs Table:0.047\n",
            " p28: ✅ ✅ | Text:0.842 vs Table:0.321\n",
            " p29: ✅ ✅ | Text:0.953 vs Table:0.053\n",
            " p30: ✅ ❌ | Text:0.917 vs Table:0.057\n",
            " p31: ✅ ❌ | Text:0.952 vs Table:0.029\n",
            " p32: ✅ ❌ | Text:0.987 vs Table:0.018\n",
            " p33: ✅ ✅ | Text:0.951 vs Table:0.035\n",
            " p34: ✅ ✅ | Text:0.934 vs Table:0.011\n",
            " p35: ✅ ❌ | Text:0.939 vs Table:0.139\n",
            " p36: ✅ ❌ | Text:0.919 vs Table:0.168\n",
            " p37: ✅ ❌ | Text:0.890 vs Table:0.287\n",
            " p38: ✅ ❌ | Text:0.982 vs Table:0.125\n",
            " p39: ✅ ✅ | Text:0.950 vs Table:0.018\n",
            " p40: ✅ ✅ | Text:0.966 vs Table:0.034\n",
            " p41: ✅ ❌ | Text:0.970 vs Table:0.383\n",
            " p42: ✅ ✅ | Text:0.914 vs Table:0.576\n",
            " p43: ✅ ✅ | Text:0.930 vs Table:0.662\n",
            " p44: ✅ ✅ | Text:0.958 vs Table:0.728\n",
            " p45: ✅ ✅ | Text:0.961 vs Table:0.541\n",
            " p46: ✅ ✅ | Text:0.886 vs Table:0.117\n",
            " p47: ✅ ✅ | Text:0.897 vs Table:0.580\n",
            " p48: ✅ ✅ | Text:0.865 vs Table:0.630\n",
            " p49: ✅ ✅ | Text:0.783 vs Table:0.899\n",
            "\n",
            "📊 Résultats CLIP:\n",
            "F1 Text: 1.000\n",
            "F1 Table: 0.483\n",
            "F1 Macro: 0.741\n",
            "Hamming loss: 0.153\n",
            "Jaccard: 0.659 & 0.789\n",
            "Jaccard samples :0.847\n",
            "Jaccard per class: [1.         0.31818182]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Amélioration en essayant consensus multi prompt\n",
        "\n",
        "Note : j'avais essayé le multi prompt sur texte également mais les résultats étaient nettement moins bon, je suis revenu uniquement sur la baseline."
      ],
      "metadata": {
        "id": "bsE9ko9NNaFX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_multilabel_clip_consensus(image_path, processor, model, model_name='CLIP 32 Consensus'):\n",
        "  \"\"\"CLIP avec consensus multi-prompts\"\"\"\n",
        "  image = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "  # Test de différents prompts par classe\n",
        "  prompts_variants = {\n",
        "        \"text\": [\n",
        "            \"document with printed text and readable content\"                   # Baseline\n",
        "        ]\n",
        "        ,\"table\": [\n",
        "            \"document with structured tables and organized data\"               # Baseline\n",
        "            ,\"page containing data tables with rows and columns\"               # Descriptif\n",
        "            ,\"administrative form with budget tables and financial data\"       # Contexte métier\n",
        "            ,\"document with tabular information and structured data layout\"     # Layout focus\n",
        "            ,\"page showing organized data grids and numerical tables\"           # Data focus\n",
        "            ,\"document containing structured information in table format\"        # Format focus\n",
        "        ]\n",
        "    }\n",
        "\n",
        "  results = {}\n",
        "  device = next(model.parameters()).device\n",
        "\n",
        "  for label, prompts_list in prompts_variants.items():\n",
        "\n",
        "    scores = []\n",
        "\n",
        "    for prompt in prompts_list:\n",
        "      # Prompts binaires pour chaque classe\n",
        "      binary_prompts = [prompt, f\"document without {label}\"]\n",
        "\n",
        "      inputs = processor(text=binary_prompts\n",
        "                         ,images=image\n",
        "                         ,return_tensors=\"pt\"\n",
        "                         ,padding=True)\n",
        "      inputs = {key: value.to(device) for key, value in inputs.items()}\n",
        "\n",
        "      with torch.no_grad(): # économie de mémoire\n",
        "        outputs = model(**inputs)\n",
        "        probs = outputs.logits_per_image.softmax(dim=1)\n",
        "\n",
        "      score = float(probs[0][0])\n",
        "      scores.append(score)\n",
        "\n",
        "    # Calcul de la moyenne des scores (consensus)\n",
        "    consensus_score = sum(scores) / len(scores)\n",
        "    results[label] = consensus_score\n",
        "\n",
        "    # debug print : affiche le détail des scores\n",
        "    # print(f\"   {label}: {scores} → avg: {consensus_score:.3f}\")\n",
        "\n",
        "  # Seuils baseline\n",
        "  thresholds = {\"text\": 0.5, \"table\": 0.5}\n",
        "  predictions = {label: score > thresholds[label] for label, score in results.items()}\n",
        "\n",
        "  return {\n",
        "        \"model\": model_name\n",
        "        ,\"predictions\": predictions\n",
        "        ,\"scores\": results\n",
        "        ,\"thresholds\": thresholds\n",
        "    }\n"
      ],
      "metadata": {
        "id": "2hUQj6jR3oFO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_clip_consensus = evaluate_binary_multilabel_model(\n",
        "    path_folder_test_baseline\n",
        "    ,ground_truth_pu_p01_pp01_multilabel\n",
        "    ,detect_multilabel_clip_consensus\n",
        "    ,processor_clip\n",
        "    ,model_clip\n",
        ")\n",
        "\n",
        "print(f\"\\n📊 Résultats CLIP:\")\n",
        "print(f\"F1 Text: {metrics_clip_consensus['f1_text']:.3f}\")\n",
        "print(f\"F1 Table: {metrics_clip_consensus['f1_table']:.3f}\")\n",
        "print(f\"F1 Macro: {metrics_clip_consensus['f1_macro']:.3f}\")\n",
        "print(f\"Hamming loss: {metrics_clip_consensus['hamming_loss']:.3f}\")\n",
        "print(f\"Jaccard: {metrics_clip_consensus['jaccard_macro']:.3f} & {metrics_clip_consensus['jaccard_micro']:.3f}\")\n",
        "print(f\"Jaccard samples :{metrics_clip_consensus['jaccard_samples']:.3f}\")\n",
        "print(f\"Jaccard per class: {metrics_clip_consensus['jaccard_per_class']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NBwwEQNR6TQC",
        "outputId": "fc9d4451-7e65-44af-a429-9e5cb9db5877"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " p1: ✅ ✅ | Text:0.983 vs Table:0.231\n",
            " p2: ✅ ✅ | Text:0.934 vs Table:0.070\n",
            " p3: ✅ ❌ | Text:0.978 vs Table:0.317\n",
            " p4: ✅ ✅ | Text:0.964 vs Table:0.191\n",
            " p5: ✅ ✅ | Text:0.957 vs Table:0.344\n",
            " p6: ✅ ✅ | Text:0.969 vs Table:0.122\n",
            " p7: ✅ ✅ | Text:0.818 vs Table:0.383\n",
            " p8: ✅ ❌ | Text:0.889 vs Table:0.303\n",
            " p9: ✅ ✅ | Text:0.968 vs Table:0.130\n",
            " p10: ✅ ✅ | Text:0.962 vs Table:0.086\n",
            " p11: ✅ ✅ | Text:0.733 vs Table:0.097\n",
            " p12: ✅ ✅ | Text:0.948 vs Table:0.578\n",
            " p13: ✅ ❌ | Text:0.960 vs Table:0.392\n",
            " p14: ✅ ✅ | Text:0.976 vs Table:0.260\n",
            " p15: ✅ ❌ | Text:0.905 vs Table:0.121\n",
            " p16: ✅ ✅ | Text:0.941 vs Table:0.173\n",
            " p17: ✅ ✅ | Text:0.969 vs Table:0.117\n",
            " p18: ✅ ✅ | Text:0.968 vs Table:0.187\n",
            " p19: ✅ ✅ | Text:0.930 vs Table:0.144\n",
            " p20: ✅ ✅ | Text:0.975 vs Table:0.258\n",
            " p21: ✅ ✅ | Text:0.923 vs Table:0.239\n",
            " p22: ✅ ❌ | Text:0.873 vs Table:0.253\n",
            " p23: ✅ ✅ | Text:0.859 vs Table:0.581\n",
            " p24: ✅ ✅ | Text:0.884 vs Table:0.209\n",
            " p25: ✅ ✅ | Text:0.937 vs Table:0.148\n",
            " p26: ✅ ✅ | Text:0.964 vs Table:0.199\n",
            " p27: ✅ ✅ | Text:0.942 vs Table:0.210\n",
            " p28: ✅ ✅ | Text:0.842 vs Table:0.424\n",
            " p29: ✅ ✅ | Text:0.953 vs Table:0.149\n",
            " p30: ✅ ❌ | Text:0.917 vs Table:0.427\n",
            " p31: ✅ ❌ | Text:0.952 vs Table:0.209\n",
            " p32: ✅ ❌ | Text:0.987 vs Table:0.097\n",
            " p33: ✅ ✅ | Text:0.951 vs Table:0.219\n",
            " p34: ✅ ✅ | Text:0.934 vs Table:0.059\n",
            " p35: ✅ ❌ | Text:0.939 vs Table:0.296\n",
            " p36: ✅ ❌ | Text:0.919 vs Table:0.288\n",
            " p37: ✅ ❌ | Text:0.890 vs Table:0.452\n",
            " p38: ✅ ❌ | Text:0.982 vs Table:0.321\n",
            " p39: ✅ ✅ | Text:0.950 vs Table:0.097\n",
            " p40: ✅ ✅ | Text:0.966 vs Table:0.289\n",
            " p41: ✅ ✅ | Text:0.970 vs Table:0.599\n",
            " p42: ✅ ✅ | Text:0.914 vs Table:0.653\n",
            " p43: ✅ ✅ | Text:0.930 vs Table:0.738\n",
            " p44: ✅ ✅ | Text:0.958 vs Table:0.766\n",
            " p45: ✅ ✅ | Text:0.961 vs Table:0.643\n",
            " p46: ✅ ✅ | Text:0.886 vs Table:0.163\n",
            " p47: ✅ ✅ | Text:0.897 vs Table:0.545\n",
            " p48: ✅ ✅ | Text:0.865 vs Table:0.600\n",
            " p49: ✅ ✅ | Text:0.783 vs Table:0.806\n",
            "\n",
            "📊 Résultats CLIP:\n",
            "F1 Text: 1.000\n",
            "F1 Table: 0.625\n",
            "F1 Macro: 0.812\n",
            "Hamming loss: 0.122\n",
            "Jaccard: 0.727 & 0.831\n",
            "Jaccard samples :0.878\n",
            "Jaccard per class: [1.         0.45454545]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Consensus v2 : test avec d'autres prompts (chatGPT)"
      ],
      "metadata": {
        "id": "H0nv0DPZOCNh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_multilabel_clip_consensus_v2(image_path, processor, model, model_name='CLIP 32 Consensus'):\n",
        "  \"\"\"CLIP avec consensus multi-prompts\"\"\"\n",
        "  image = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "  # Test de différents prompts par classe\n",
        "  prompts_variants = {\n",
        "        \"text\": [\n",
        "            \"document with printed text and readable content\"                   # Baseline\n",
        "        ]\n",
        "        ,\"table\": [\n",
        "            \"document with data tables containing rows and columns\"\n",
        "            ,\"administrative document with structured numerical tables\"\n",
        "            ,\"page showing budget tables and financial data\"\n",
        "            ,\"document containing organized tabular information\"\n",
        "            ,\"form with data grids and structured layouts\"\n",
        "            ,\"document with statistical tables and data charts\"\n",
        "        ]\n",
        "    }\n",
        "\n",
        "  results = {}\n",
        "  device = next(model.parameters()).device\n",
        "\n",
        "  for label, prompts_list in prompts_variants.items():\n",
        "\n",
        "    scores = []\n",
        "\n",
        "    for prompt in prompts_list:\n",
        "      # Prompts binaires pour chaque classe\n",
        "      binary_prompts = [prompt, f\"document without {label}\"]\n",
        "\n",
        "      inputs = processor(text=binary_prompts\n",
        "                         ,images=image\n",
        "                         ,return_tensors=\"pt\"\n",
        "                         ,padding=True)\n",
        "      inputs = {key: value.to(device) for key, value in inputs.items()}\n",
        "\n",
        "      with torch.no_grad(): # économie de mémoire\n",
        "        outputs = model(**inputs)\n",
        "        probs = outputs.logits_per_image.softmax(dim=1)\n",
        "\n",
        "      score = float(probs[0][0])\n",
        "      scores.append(score)\n",
        "\n",
        "    # Calcul de la moyenne des scores (consensus)\n",
        "    consensus_score = sum(scores) / len(scores)\n",
        "    results[label] = consensus_score\n",
        "\n",
        "    # debug print : affiche le détail des scores\n",
        "    # print(f\"   {label}: {scores} → avg: {consensus_score:.3f}\")\n",
        "\n",
        "  # Seuils baseline\n",
        "  thresholds = {\"text\": 0.5, \"table\": 0.5}\n",
        "  predictions = {label: score > thresholds[label] for label, score in results.items()}\n",
        "\n",
        "  return {\n",
        "        \"model\": model_name\n",
        "        ,\"predictions\": predictions\n",
        "        ,\"scores\": results\n",
        "        ,\"thresholds\": thresholds\n",
        "    }\n"
      ],
      "metadata": {
        "id": "Vl0SFr2t9mK1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_clip_consensus_v2 = evaluate_binary_multilabel_model(\n",
        "    path_folder_test_baseline\n",
        "    ,ground_truth_pu_p01_pp01_multilabel\n",
        "    ,detect_multilabel_clip_consensus_v2\n",
        "    ,processor_clip\n",
        "    ,model_clip\n",
        ")\n",
        "\n",
        "print(f\"\\n📊 Résultats CLIP:\")\n",
        "print(f\"F1 Text: {metrics_clip_consensus['f1_text']:.3f}\")\n",
        "print(f\"F1 Table: {metrics_clip_consensus['f1_table']:.3f}\")\n",
        "print(f\"F1 Macro: {metrics_clip_consensus['f1_macro']:.3f}\")\n",
        "print(f\"Hamming loss: {metrics_clip_consensus['hamming_loss']:.3f}\")\n",
        "print(f\"Jaccard: {metrics_clip_consensus['jaccard_macro']:.3f} & {metrics_clip_consensus['jaccard_micro']:.3f}\")\n",
        "print(f\"Jaccard samples :{metrics_clip_consensus['jaccard_samples']:.3f}\")\n",
        "print(f\"Jaccard per class: {metrics_clip_consensus['jaccard_per_class']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Lwy4d5f-tAO",
        "outputId": "8139e2fa-71de-45b6-e9c1-7d46dbdb2660"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " p1: ✅ ✅ | Text:0.983 vs Table:0.332\n",
            " p2: ✅ ✅ | Text:0.934 vs Table:0.092\n",
            " p3: ✅ ❌ | Text:0.978 vs Table:0.358\n",
            " p4: ✅ ✅ | Text:0.964 vs Table:0.370\n",
            " p5: ✅ ✅ | Text:0.957 vs Table:0.245\n",
            " p6: ✅ ✅ | Text:0.969 vs Table:0.123\n",
            " p7: ✅ ❌ | Text:0.818 vs Table:0.542\n",
            " p8: ✅ ❌ | Text:0.889 vs Table:0.276\n",
            " p9: ✅ ✅ | Text:0.968 vs Table:0.107\n",
            " p10: ✅ ✅ | Text:0.962 vs Table:0.129\n",
            " p11: ✅ ✅ | Text:0.733 vs Table:0.162\n",
            " p12: ✅ ✅ | Text:0.948 vs Table:0.567\n",
            " p13: ✅ ❌ | Text:0.960 vs Table:0.364\n",
            " p14: ✅ ✅ | Text:0.976 vs Table:0.194\n",
            " p15: ✅ ❌ | Text:0.905 vs Table:0.169\n",
            " p16: ✅ ✅ | Text:0.941 vs Table:0.280\n",
            " p17: ✅ ✅ | Text:0.969 vs Table:0.104\n",
            " p18: ✅ ✅ | Text:0.968 vs Table:0.221\n",
            " p19: ✅ ✅ | Text:0.930 vs Table:0.127\n",
            " p20: ✅ ✅ | Text:0.975 vs Table:0.258\n",
            " p21: ✅ ✅ | Text:0.923 vs Table:0.263\n",
            " p22: ✅ ❌ | Text:0.873 vs Table:0.244\n",
            " p23: ✅ ✅ | Text:0.859 vs Table:0.639\n",
            " p24: ✅ ✅ | Text:0.884 vs Table:0.220\n",
            " p25: ✅ ✅ | Text:0.937 vs Table:0.106\n",
            " p26: ✅ ✅ | Text:0.964 vs Table:0.178\n",
            " p27: ✅ ✅ | Text:0.942 vs Table:0.189\n",
            " p28: ✅ ✅ | Text:0.842 vs Table:0.342\n",
            " p29: ✅ ✅ | Text:0.953 vs Table:0.240\n",
            " p30: ✅ ❌ | Text:0.917 vs Table:0.472\n",
            " p31: ✅ ❌ | Text:0.952 vs Table:0.246\n",
            " p32: ✅ ❌ | Text:0.987 vs Table:0.135\n",
            " p33: ✅ ✅ | Text:0.951 vs Table:0.162\n",
            " p34: ✅ ✅ | Text:0.934 vs Table:0.097\n",
            " p35: ✅ ❌ | Text:0.939 vs Table:0.297\n",
            " p36: ✅ ❌ | Text:0.919 vs Table:0.291\n",
            " p37: ✅ ❌ | Text:0.890 vs Table:0.417\n",
            " p38: ✅ ❌ | Text:0.982 vs Table:0.330\n",
            " p39: ✅ ✅ | Text:0.950 vs Table:0.104\n",
            " p40: ✅ ✅ | Text:0.966 vs Table:0.217\n",
            " p41: ✅ ✅ | Text:0.970 vs Table:0.703\n",
            " p42: ✅ ✅ | Text:0.914 vs Table:0.679\n",
            " p43: ✅ ✅ | Text:0.930 vs Table:0.759\n",
            " p44: ✅ ✅ | Text:0.958 vs Table:0.788\n",
            " p45: ✅ ✅ | Text:0.961 vs Table:0.685\n",
            " p46: ✅ ✅ | Text:0.886 vs Table:0.228\n",
            " p47: ✅ ✅ | Text:0.897 vs Table:0.685\n",
            " p48: ✅ ✅ | Text:0.865 vs Table:0.726\n",
            " p49: ✅ ✅ | Text:0.783 vs Table:0.809\n",
            "\n",
            "📊 Résultats CLIP:\n",
            "F1 Text: 1.000\n",
            "F1 Table: 0.625\n",
            "F1 Macro: 0.812\n",
            "Hamming loss: 0.122\n",
            "Jaccard: 0.727 & 0.831\n",
            "Jaccard samples :0.878\n",
            "Jaccard per class: [1.         0.45454545]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analyse des prompts pour sélectionner les meilleurs prompts"
      ],
      "metadata": {
        "id": "N4ejvfJGOOsY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_prompt_effectiveness(image_path, processor, model, label=\"table\"):\n",
        "    \"\"\"Analyse quel prompt fonctionne le mieux\"\"\"\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "    if label == \"table\":\n",
        "        prompts = [\n",
        "            \"document with structured tables and organized data\"                # Baseline\n",
        "            ,\"page containing data tables with rows and columns\"                # Descriptif\n",
        "            ,\"administrative form with budget tables and financial data\"        # Contexte métier\n",
        "            ,\"document with tabular information and structured data layout\"     # Layout focus\n",
        "            ,\"page showing organized data grids and numerical tables\"           # Data focus\n",
        "            ,\"document containing structured information in table format\"       # Format focus\n",
        "        ]\n",
        "    else:  # text\n",
        "        prompts = [\n",
        "            \"document with printed text and readable content\"\n",
        "            ,\"administrative document containing written text\"\n",
        "            ,\"page with paragraphs and textual information\"\n",
        "            ,\"document showing readable text content and written material\"\n",
        "            ,\"form with written instructions and text sections\"\n",
        "        ]\n",
        "\n",
        "    results = []\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    for i, prompt in enumerate(prompts):\n",
        "        binary_prompts = [prompt, f\"document without {label}\"]\n",
        "\n",
        "        inputs = processor(text=binary_prompts, images=image, return_tensors=\"pt\", padding=True)\n",
        "        inputs = {key: value.to(device) for key, value in inputs.items()}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "            probs = outputs.logits_per_image.softmax(dim=1)\n",
        "\n",
        "        score = float(probs[0][0])\n",
        "        results.append((i, prompt[:50] + \"...\", score))\n",
        "\n",
        "    # Tri par efficacité\n",
        "    results.sort(key=lambda x: x[2], reverse=True)\n",
        "    return results"
      ],
      "metadata": {
        "id": "0uHI5sra_--N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### page qui nécessite une amélioration ###\n",
        "\n",
        "for page in [\"p3\"\n",
        "            ,\"p8\"\n",
        "            ,\"p13\"\n",
        "            ,\"p15\"\n",
        "            ,\"p22\"\n",
        "            ,\"p30\"\n",
        "            ,\"p31\"\n",
        "            ,\"p32\"\n",
        "            ,\"p35\"\n",
        "            ,\"p36\"\n",
        "            ,\"p37\"\n",
        "            ,\"p38\"]:\n",
        "    print(f\"\\n📊 {page} - Analysis table prompts:\")\n",
        "    analysis = analyze_prompt_effectiveness(path_pu_p01_pp01[page], processor_clip, model_clip, \"table\")\n",
        "    for rank, (idx, prompt, score) in enumerate(analysis):\n",
        "        print(f\"  {rank+1}. [{idx}] {score:.3f} - {prompt}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yY4DTLXkAB8B",
        "outputId": "582baaf7-2081-4e91-8ef2-33f9a14901ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📊 p3 - Analysis table prompts:\n",
            "  1. [3] 0.727 - document with tabular information and structured d...\n",
            "  2. [2] 0.461 - administrative form with budget tables and financi...\n",
            "  3. [5] 0.348 - document containing structured information in tabl...\n",
            "  4. [4] 0.236 - page showing organized data grids and numerical ta...\n",
            "  5. [0] 0.106 - document with structured tables and organized data...\n",
            "  6. [1] 0.027 - page containing data tables with rows and columns...\n",
            "\n",
            "📊 p8 - Analysis table prompts:\n",
            "  1. [2] 0.679 - administrative form with budget tables and financi...\n",
            "  2. [5] 0.578 - document containing structured information in tabl...\n",
            "  3. [3] 0.272 - document with tabular information and structured d...\n",
            "  4. [4] 0.214 - page showing organized data grids and numerical ta...\n",
            "  5. [0] 0.053 - document with structured tables and organized data...\n",
            "  6. [1] 0.019 - page containing data tables with rows and columns...\n",
            "\n",
            "📊 p13 - Analysis table prompts:\n",
            "  1. [4] 0.799 - page showing organized data grids and numerical ta...\n",
            "  2. [3] 0.561 - document with tabular information and structured d...\n",
            "  3. [2] 0.443 - administrative form with budget tables and financi...\n",
            "  4. [5] 0.384 - document containing structured information in tabl...\n",
            "  5. [0] 0.120 - document with structured tables and organized data...\n",
            "  6. [1] 0.044 - page containing data tables with rows and columns...\n",
            "\n",
            "📊 p15 - Analysis table prompts:\n",
            "  1. [2] 0.285 - administrative form with budget tables and financi...\n",
            "  2. [4] 0.230 - page showing organized data grids and numerical ta...\n",
            "  3. [3] 0.173 - document with tabular information and structured d...\n",
            "  4. [0] 0.027 - document with structured tables and organized data...\n",
            "  5. [5] 0.013 - document containing structured information in tabl...\n",
            "  6. [1] 0.001 - page containing data tables with rows and columns...\n",
            "\n",
            "📊 p22 - Analysis table prompts:\n",
            "  1. [2] 0.774 - administrative form with budget tables and financi...\n",
            "  2. [5] 0.437 - document containing structured information in tabl...\n",
            "  3. [3] 0.219 - document with tabular information and structured d...\n",
            "  4. [4] 0.053 - page showing organized data grids and numerical ta...\n",
            "  5. [0] 0.024 - document with structured tables and organized data...\n",
            "  6. [1] 0.010 - page containing data tables with rows and columns...\n",
            "\n",
            "📊 p30 - Analysis table prompts:\n",
            "  1. [5] 0.770 - document containing structured information in tabl...\n",
            "  2. [4] 0.696 - page showing organized data grids and numerical ta...\n",
            "  3. [2] 0.509 - administrative form with budget tables and financi...\n",
            "  4. [3] 0.409 - document with tabular information and structured d...\n",
            "  5. [1] 0.120 - page containing data tables with rows and columns...\n",
            "  6. [0] 0.057 - document with structured tables and organized data...\n",
            "\n",
            "📊 p31 - Analysis table prompts:\n",
            "  1. [2] 0.566 - administrative form with budget tables and financi...\n",
            "  2. [5] 0.289 - document containing structured information in tabl...\n",
            "  3. [4] 0.262 - page showing organized data grids and numerical ta...\n",
            "  4. [3] 0.088 - document with tabular information and structured d...\n",
            "  5. [0] 0.029 - document with structured tables and organized data...\n",
            "  6. [1] 0.021 - page containing data tables with rows and columns...\n",
            "\n",
            "📊 p32 - Analysis table prompts:\n",
            "  1. [3] 0.242 - document with tabular information and structured d...\n",
            "  2. [5] 0.195 - document containing structured information in tabl...\n",
            "  3. [2] 0.084 - administrative form with budget tables and financi...\n",
            "  4. [4] 0.040 - page showing organized data grids and numerical ta...\n",
            "  5. [0] 0.018 - document with structured tables and organized data...\n",
            "  6. [1] 0.004 - page containing data tables with rows and columns...\n",
            "\n",
            "📊 p35 - Analysis table prompts:\n",
            "  1. [2] 0.568 - administrative form with budget tables and financi...\n",
            "  2. [5] 0.480 - document containing structured information in tabl...\n",
            "  3. [4] 0.260 - page showing organized data grids and numerical ta...\n",
            "  4. [3] 0.218 - document with tabular information and structured d...\n",
            "  5. [0] 0.139 - document with structured tables and organized data...\n",
            "  6. [1] 0.110 - page containing data tables with rows and columns...\n",
            "\n",
            "📊 p36 - Analysis table prompts:\n",
            "  1. [2] 0.539 - administrative form with budget tables and financi...\n",
            "  2. [5] 0.375 - document containing structured information in tabl...\n",
            "  3. [4] 0.326 - page showing organized data grids and numerical ta...\n",
            "  4. [3] 0.217 - document with tabular information and structured d...\n",
            "  5. [0] 0.168 - document with structured tables and organized data...\n",
            "  6. [1] 0.104 - page containing data tables with rows and columns...\n",
            "\n",
            "📊 p37 - Analysis table prompts:\n",
            "  1. [2] 0.670 - administrative form with budget tables and financi...\n",
            "  2. [5] 0.595 - document containing structured information in tabl...\n",
            "  3. [4] 0.473 - page showing organized data grids and numerical ta...\n",
            "  4. [1] 0.356 - page containing data tables with rows and columns...\n",
            "  5. [3] 0.334 - document with tabular information and structured d...\n",
            "  6. [0] 0.287 - document with structured tables and organized data...\n",
            "\n",
            "📊 p38 - Analysis table prompts:\n",
            "  1. [3] 0.504 - document with tabular information and structured d...\n",
            "  2. [5] 0.457 - document containing structured information in tabl...\n",
            "  3. [4] 0.423 - page showing organized data grids and numerical ta...\n",
            "  4. [2] 0.398 - administrative form with budget tables and financi...\n",
            "  5. [0] 0.125 - document with structured tables and organized data...\n",
            "  6. [1] 0.020 - page containing data tables with rows and columns...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Consensus v3 : réduction du nombre de prompt suite analyse"
      ],
      "metadata": {
        "id": "4gXhBRhXOqgi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_multilabel_clip_consensus_v3(image_path, processor, model, model_name='CLIP 32 Consensus'):\n",
        "  \"\"\"CLIP avec consensus multi-prompts\"\"\"\n",
        "  image = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "  # Test de différents prompts par classe\n",
        "  prompts_variants = {\n",
        "        \"text\": [\n",
        "            \"document with printed text and readable content\"                   # Baseline\n",
        "        ]\n",
        "        ,\"table\": [\n",
        "            \"administrative form with budget tables and financial data\"\n",
        "            ,\"document with tabular information and structured data\"\n",
        "            ,\"document containing structured information in table format\"\n",
        "        ]\n",
        "    }\n",
        "\n",
        "  results = {}\n",
        "  device = next(model.parameters()).device\n",
        "\n",
        "  for label, prompts_list in prompts_variants.items():\n",
        "\n",
        "    scores = []\n",
        "\n",
        "    for prompt in prompts_list:\n",
        "      # Prompts binaires pour chaque classe\n",
        "      binary_prompts = [prompt, f\"document without {label}\"]\n",
        "\n",
        "      inputs = processor(text=binary_prompts\n",
        "                         ,images=image\n",
        "                         ,return_tensors=\"pt\"\n",
        "                         ,padding=True)\n",
        "      inputs = {key: value.to(device) for key, value in inputs.items()}\n",
        "\n",
        "      with torch.no_grad(): # économie de mémoire\n",
        "        outputs = model(**inputs)\n",
        "        probs = outputs.logits_per_image.softmax(dim=1)\n",
        "\n",
        "      score = float(probs[0][0])\n",
        "      scores.append(score)\n",
        "\n",
        "    # Calcul de la moyenne des scores (consensus)\n",
        "    consensus_score = sum(scores) / len(scores)\n",
        "    results[label] = consensus_score\n",
        "\n",
        "    # debug print : affiche le détail des scores\n",
        "    # print(f\"   {label}: {scores} → avg: {consensus_score:.3f}\")\n",
        "\n",
        "  # Seuils baseline\n",
        "  thresholds = {\"text\": 0.5, \"table\": 0.5}\n",
        "  predictions = {label: score > thresholds[label] for label, score in results.items()}\n",
        "\n",
        "  return {\n",
        "        \"model\": model_name\n",
        "        ,\"predictions\": predictions\n",
        "        ,\"scores\": results\n",
        "        ,\"thresholds\": thresholds\n",
        "    }\n"
      ],
      "metadata": {
        "id": "DEoVqp9sBvFs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_clip_consensus_v3 = evaluate_binary_multilabel_model(\n",
        "    path_folder_test_baseline\n",
        "    ,ground_truth_pu_p01_pp01_multilabel\n",
        "    ,detect_multilabel_clip_consensus_v3\n",
        "    ,processor_clip\n",
        "    ,model_clip\n",
        ")\n",
        "\n",
        "print(f\"\\n📊 Résultats CLIP:\")\n",
        "print(f\"F1 Text: {metrics_clip_consensus_v3['f1_text']:.3f}\")\n",
        "print(f\"F1 Table: {metrics_clip_consensus_v3['f1_table']:.3f}\")\n",
        "print(f\"F1 Macro: {metrics_clip_consensus_v3['f1_macro']:.3f}\")\n",
        "print(f\"Hamming loss: {metrics_clip_consensus_v3['hamming_loss']:.3f}\")\n",
        "print(f\"Jaccard: {metrics_clip_consensus_v3['jaccard_macro']:.3f} & {metrics_clip_consensus_v3['jaccard_micro']:.3f}\")\n",
        "print(f\"Jaccard samples :{metrics_clip_consensus_v3['jaccard_samples']:.3f}\")\n",
        "print(f\"Jaccard per class: {metrics_clip_consensus_v3['jaccard_per_class']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8V0TZsBFCEgK",
        "outputId": "842d7a0a-bbec-4166-f851-d297a9191da8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " p1: ✅ ✅ | Text:0.983 vs Table:0.417\n",
            " p2: ✅ ✅ | Text:0.934 vs Table:0.117\n",
            " p3: ✅ ❌ | Text:0.978 vs Table:0.484\n",
            " p4: ✅ ✅ | Text:0.964 vs Table:0.309\n",
            " p5: ✅ ❌ | Text:0.957 vs Table:0.597\n",
            " p6: ✅ ✅ | Text:0.969 vs Table:0.229\n",
            " p7: ✅ ✅ | Text:0.818 vs Table:0.452\n",
            " p8: ✅ ❌ | Text:0.889 vs Table:0.481\n",
            " p9: ✅ ✅ | Text:0.968 vs Table:0.223\n",
            " p10: ✅ ✅ | Text:0.962 vs Table:0.124\n",
            " p11: ✅ ✅ | Text:0.733 vs Table:0.175\n",
            " p12: ✅ ✅ | Text:0.948 vs Table:0.802\n",
            " p13: ✅ ❌ | Text:0.960 vs Table:0.487\n",
            " p14: ✅ ✅ | Text:0.976 vs Table:0.466\n",
            " p15: ✅ ❌ | Text:0.905 vs Table:0.201\n",
            " p16: ✅ ✅ | Text:0.941 vs Table:0.309\n",
            " p17: ✅ ✅ | Text:0.969 vs Table:0.196\n",
            " p18: ✅ ✅ | Text:0.968 vs Table:0.294\n",
            " p19: ✅ ✅ | Text:0.930 vs Table:0.240\n",
            " p20: ✅ ✅ | Text:0.975 vs Table:0.496\n",
            " p21: ✅ ✅ | Text:0.923 vs Table:0.403\n",
            " p22: ✅ ❌ | Text:0.873 vs Table:0.456\n",
            " p23: ✅ ✅ | Text:0.859 vs Table:0.720\n",
            " p24: ✅ ✅ | Text:0.884 vs Table:0.357\n",
            " p25: ✅ ✅ | Text:0.937 vs Table:0.284\n",
            " p26: ✅ ✅ | Text:0.964 vs Table:0.394\n",
            " p27: ✅ ✅ | Text:0.942 vs Table:0.398\n",
            " p28: ✅ ❌ | Text:0.842 vs Table:0.611\n",
            " p29: ✅ ✅ | Text:0.953 vs Table:0.200\n",
            " p30: ✅ ✅ | Text:0.917 vs Table:0.502\n",
            " p31: ✅ ❌ | Text:0.952 vs Table:0.316\n",
            " p32: ✅ ❌ | Text:0.987 vs Table:0.193\n",
            " p33: ✅ ✅ | Text:0.951 vs Table:0.380\n",
            " p34: ✅ ✅ | Text:0.934 vs Table:0.104\n",
            " p35: ✅ ❌ | Text:0.939 vs Table:0.418\n",
            " p36: ✅ ❌ | Text:0.919 vs Table:0.369\n",
            " p37: ✅ ✅ | Text:0.890 vs Table:0.517\n",
            " p38: ✅ ❌ | Text:0.982 vs Table:0.420\n",
            " p39: ✅ ✅ | Text:0.950 vs Table:0.183\n",
            " p40: ✅ ❌ | Text:0.966 vs Table:0.501\n",
            " p41: ✅ ✅ | Text:0.970 vs Table:0.749\n",
            " p42: ✅ ✅ | Text:0.914 vs Table:0.749\n",
            " p43: ✅ ✅ | Text:0.930 vs Table:0.875\n",
            " p44: ✅ ✅ | Text:0.958 vs Table:0.829\n",
            " p45: ✅ ✅ | Text:0.961 vs Table:0.778\n",
            " p46: ✅ ✅ | Text:0.886 vs Table:0.329\n",
            " p47: ✅ ✅ | Text:0.897 vs Table:0.629\n",
            " p48: ✅ ✅ | Text:0.865 vs Table:0.651\n",
            " p49: ✅ ✅ | Text:0.783 vs Table:0.823\n",
            "\n",
            "📊 Résultats CLIP:\n",
            "F1 Text: 1.000\n",
            "F1 Table: 0.649\n",
            "F1 Macro: 0.824\n",
            "Hamming loss: 0.133\n",
            "Jaccard: 0.740 & 0.824\n",
            "Jaccard samples :0.867\n",
            "Jaccard per class: [1.   0.48]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Recherche spécifique pour p15, essai sur Consensus v3_1"
      ],
      "metadata": {
        "id": "72V_kzVuRXPF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "j37ZutHzRJEL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_prompt_effectiveness_p15(image_path, processor, model):\n",
        "    \"\"\"Version spécialisée de l'analyse de prompts pour p15\"\"\"\n",
        "\n",
        "    # Focus sur les prompts qui pourraient mieux marcher pour p15\n",
        "    test_prompts = [\n",
        "        # Baseline actuel\n",
        "    \"business process flowchart with colored sections and directional arrows, not planning schedule\",\n",
        "    # Focus éléments visuels uniques aux vrais diagrammes\n",
        "    \"flowchart with interconnected boxes and directional flow arrows\",\n",
        "    \"diagram showing process relationships with connecting arrows\",\n",
        "    \"business logic diagram with linked components and flow direction\",\n",
        "\n",
        "    # Exclusion explicite planning\n",
        "    \"process flowchart with connections, not scheduling document\",\n",
        "    \"conceptual diagram with linked elements, not timeline or calendar\",\n",
        "    \"workflow diagram with process flow, not planning grid\",\n",
        "\n",
        "    # Focus connexions conceptuelles\n",
        "    \"conceptual framework diagram with connected logical elements\",\n",
        "    \"process model showing relationships between different components\"\n",
        "    ]\n",
        "\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    scores = []\n",
        "\n",
        "    print(\"🔬 ANALYSE SPÉCIALISÉE p15\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    for prompt in test_prompts:\n",
        "        # Test binaire pour chaque prompt\n",
        "        binary_prompts = [prompt, f\"document with table\"]\n",
        "\n",
        "        inputs = processor(text=binary_prompts, images=image,\n",
        "                         return_tensors=\"pt\", padding=True)\n",
        "        inputs = {key: value.to(device) for key, value in inputs.items()}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "            probs = outputs.logits_per_image.softmax(dim=1)\n",
        "\n",
        "        score = float(probs[0][0])\n",
        "        scores.append((score, prompt))\n",
        "        print(f\"   {score:.3f} | {prompt}\")\n",
        "\n",
        "    # Tri par performance\n",
        "    sorted_scores = sorted(scores, reverse=True)\n",
        "\n",
        "    print(f\"\\n🏆 RANKING:\")\n",
        "    for i, (score, prompt) in enumerate(sorted_scores):\n",
        "        print(f\"   {i+1:2d}. {score:.3f} | {prompt}\")\n",
        "\n",
        "    return sorted_scores"
      ],
      "metadata": {
        "id": "SviZ-TZeRjeX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_multilabel_clip_consensus_v3_1(image_path, processor, model, model_name='CLIP 32 Consensus'):\n",
        "  \"\"\"CLIP avec consensus multi-prompts\"\"\"\n",
        "  image = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "  # Test de différents prompts par classe\n",
        "  prompts_variants = {\n",
        "        \"text\": [\n",
        "            \"document with printed text and readable content\"                   # Baseline\n",
        "        ]\n",
        "        ,\"table\": [\n",
        "            \"administrative form with budget tables and financial data\"\n",
        "            ,\"document with tabular information and structured data\"\n",
        "            ,\"document containing structured information in table format\"\n",
        "            ,\"infographic showing structured data relationships\"\n",
        "        ]\n",
        "    }\n",
        "\n",
        "  results = {}\n",
        "  device = next(model.parameters()).device\n",
        "\n",
        "  for label, prompts_list in prompts_variants.items():\n",
        "\n",
        "    scores = []\n",
        "\n",
        "    for prompt in prompts_list:\n",
        "      # Prompts binaires pour chaque classe\n",
        "      binary_prompts = [prompt, f\"document without {label}\"]\n",
        "\n",
        "      inputs = processor(text=binary_prompts\n",
        "                         ,images=image\n",
        "                         ,return_tensors=\"pt\"\n",
        "                         ,padding=True)\n",
        "      inputs = {key: value.to(device) for key, value in inputs.items()}\n",
        "\n",
        "      with torch.no_grad(): # économie de mémoire\n",
        "        outputs = model(**inputs)\n",
        "        probs = outputs.logits_per_image.softmax(dim=1)\n",
        "\n",
        "      score = float(probs[0][0])\n",
        "      scores.append(score)\n",
        "\n",
        "    # Calcul de la moyenne des scores (consensus)\n",
        "    consensus_score = sum(scores) / len(scores)\n",
        "    results[label] = consensus_score\n",
        "\n",
        "    # debug print : affiche le détail des scores\n",
        "    # print(f\"   {label}: {scores} → avg: {consensus_score:.3f}\")\n",
        "\n",
        "  # Seuils baseline\n",
        "  thresholds = {\"text\": 0.5, \"table\": 0.5}\n",
        "  predictions = {label: score > thresholds[label] for label, score in results.items()}\n",
        "\n",
        "  return {\n",
        "        \"model\": model_name\n",
        "        ,\"predictions\": predictions\n",
        "        ,\"scores\": results\n",
        "        ,\"thresholds\": thresholds\n",
        "    }"
      ],
      "metadata": {
        "id": "RKUgU9jga6xz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_clip_consensus_v3_1 = evaluate_binary_multilabel_model(\n",
        "    path_folder_test_baseline\n",
        "    ,ground_truth_pu_p01_pp01_multilabel\n",
        "    ,detect_multilabel_clip_consensus_v3_1\n",
        "    ,processor_clip\n",
        "    ,model_clip\n",
        ")\n",
        "\n",
        "print(f\"\\n📊 Résultats CLIP:\")\n",
        "print(f\"F1 Text: {metrics_clip_consensus_v3_1['f1_text']:.3f}\")\n",
        "print(f\"F1 Table: {metrics_clip_consensus_v3_1['f1_table']:.3f}\")\n",
        "print(f\"F1 Macro: {metrics_clip_consensus_v3_1['f1_macro']:.3f}\")\n",
        "print(f\"Hamming loss: {metrics_clip_consensus_v3_1['hamming_loss']:.3f}\")\n",
        "print(f\"Jaccard: {metrics_clip_consensus_v3_1['jaccard_macro']:.3f} & {metrics_clip_consensus_v3_1['jaccard_micro']:.3f}\")\n",
        "print(f\"Jaccard samples :{metrics_clip_consensus_v3_1['jaccard_samples']:.3f}\")\n",
        "print(f\"Jaccard per class: {metrics_clip_consensus_v3_1['jaccard_per_class']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aDCQONJFbGgb",
        "outputId": "5a919c2b-f937-4126-818c-2d8ec1bb261d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " p1: ✅ ✅ | Text:0.983 vs Table:0.314\n",
            " p2: ✅ ✅ | Text:0.934 vs Table:0.093\n",
            " p3: ✅ ❌ | Text:0.978 vs Table:0.366\n",
            " p4: ✅ ✅ | Text:0.964 vs Table:0.233\n",
            " p5: ✅ ✅ | Text:0.957 vs Table:0.458\n",
            " p6: ✅ ✅ | Text:0.969 vs Table:0.175\n",
            " p7: ✅ ✅ | Text:0.818 vs Table:0.480\n",
            " p8: ✅ ❌ | Text:0.889 vs Table:0.363\n",
            " p9: ✅ ✅ | Text:0.968 vs Table:0.173\n",
            " p10: ✅ ✅ | Text:0.962 vs Table:0.097\n",
            " p11: ✅ ✅ | Text:0.733 vs Table:0.320\n",
            " p12: ✅ ✅ | Text:0.948 vs Table:0.628\n",
            " p13: ✅ ❌ | Text:0.960 vs Table:0.466\n",
            " p14: ✅ ✅ | Text:0.976 vs Table:0.384\n",
            " p15: ✅ ❌ | Text:0.905 vs Table:0.397\n",
            " p16: ✅ ✅ | Text:0.941 vs Table:0.293\n",
            " p17: ✅ ✅ | Text:0.969 vs Table:0.149\n",
            " p18: ✅ ✅ | Text:0.968 vs Table:0.221\n",
            " p19: ✅ ✅ | Text:0.930 vs Table:0.181\n",
            " p20: ✅ ✅ | Text:0.975 vs Table:0.412\n",
            " p21: ✅ ✅ | Text:0.923 vs Table:0.304\n",
            " p22: ✅ ❌ | Text:0.873 vs Table:0.342\n",
            " p23: ✅ ✅ | Text:0.859 vs Table:0.544\n",
            " p24: ✅ ✅ | Text:0.884 vs Table:0.281\n",
            " p25: ✅ ✅ | Text:0.937 vs Table:0.257\n",
            " p26: ✅ ✅ | Text:0.964 vs Table:0.320\n",
            " p27: ✅ ✅ | Text:0.942 vs Table:0.312\n",
            " p28: ✅ ✅ | Text:0.842 vs Table:0.475\n",
            " p29: ✅ ✅ | Text:0.953 vs Table:0.205\n",
            " p30: ✅ ❌ | Text:0.917 vs Table:0.377\n",
            " p31: ✅ ❌ | Text:0.952 vs Table:0.263\n",
            " p32: ✅ ❌ | Text:0.987 vs Table:0.184\n",
            " p33: ✅ ✅ | Text:0.951 vs Table:0.290\n",
            " p34: ✅ ✅ | Text:0.934 vs Table:0.081\n",
            " p35: ✅ ❌ | Text:0.939 vs Table:0.326\n",
            " p36: ✅ ❌ | Text:0.919 vs Table:0.296\n",
            " p37: ✅ ❌ | Text:0.890 vs Table:0.411\n",
            " p38: ✅ ❌ | Text:0.982 vs Table:0.323\n",
            " p39: ✅ ✅ | Text:0.950 vs Table:0.146\n",
            " p40: ✅ ✅ | Text:0.966 vs Table:0.377\n",
            " p41: ✅ ✅ | Text:0.970 vs Table:0.617\n",
            " p42: ✅ ✅ | Text:0.914 vs Table:0.572\n",
            " p43: ✅ ✅ | Text:0.930 vs Table:0.671\n",
            " p44: ✅ ✅ | Text:0.958 vs Table:0.659\n",
            " p45: ✅ ✅ | Text:0.961 vs Table:0.627\n",
            " p46: ✅ ✅ | Text:0.886 vs Table:0.362\n",
            " p47: ✅ ✅ | Text:0.897 vs Table:0.689\n",
            " p48: ✅ ✅ | Text:0.865 vs Table:0.734\n",
            " p49: ✅ ✅ | Text:0.783 vs Table:0.798\n",
            "\n",
            "📊 Résultats CLIP:\n",
            "F1 Text: 1.000\n",
            "F1 Table: 0.625\n",
            "F1 Macro: 0.812\n",
            "Hamming loss: 0.122\n",
            "Jaccard: 0.727 & 0.831\n",
            "Jaccard samples :0.878\n",
            "Jaccard per class: [1.         0.45454545]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Thinking Note\n",
        "\n",
        "j'ai essayé l'optimisation des threshold mais ça augmentait les faux positifs.\n",
        "En baissant le seuil, on augmente le rappel et baisse la précision.\n",
        "\n",
        "Solution : Amélioration des prompts\n",
        "\n",
        "Test : Essai des poids puis GridSearch"
      ],
      "metadata": {
        "id": "_6DJDg6eSpvn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ajout des poids\n",
        "\n",
        "Test manuel"
      ],
      "metadata": {
        "id": "dKy1qPA6W8WS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_multilabel_clip_weighted_v3_1(image_path, processor, model, model_name='CLIP v3.1 Weighted'):\n",
        "    \"\"\"CLIP consensus v3.1 avec poids optimisés basés sur l'analyse\"\"\"\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "    # Poids ajustés basés sur les performances observées\n",
        "    prompts_variants = {\n",
        "        \"text\": [\n",
        "            (\"document with printed text and readable content\", 1.0)\n",
        "        ],\n",
        "        \"table\": [\n",
        "            # Prompts v3 maintenus avec poids élevés (prouvés efficaces)\n",
        "            (\"administrative form with budget tables and financial data\", 1.4)     # ↑ Excellent sur Excel\n",
        "            ,(\"document with tabular information and structured data\", 1.6)        # ↓ Très bon généraliste\n",
        "            ,(\"document containing structured information in table format\", 1.2)   # ↑ Support solide\n",
        "\n",
        "            # Prompt infographic avec poids modéré (spécialisé diagrammes)\n",
        "            ,(\"infographic showing structured data relationships\", 1.0)             # ↓ Aide p15\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    results = {}\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    for label, prompts_list in prompts_variants.items():\n",
        "        weighted_scores = []\n",
        "        total_weight = 0\n",
        "\n",
        "        for prompt, weight in prompts_list:\n",
        "            binary_prompts = [prompt, f\"document without {label}\"]\n",
        "\n",
        "            inputs = processor(text=binary_prompts\n",
        "                             ,images=image\n",
        "                             ,return_tensors=\"pt\"\n",
        "                             ,padding=True)\n",
        "            inputs = {key: value.to(device) for key, value in inputs.items()}\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = model(**inputs)\n",
        "                probs = outputs.logits_per_image.softmax(dim=1)\n",
        "\n",
        "            score = float(probs[0][0])\n",
        "            weighted_scores.append(score * weight)\n",
        "            total_weight += weight\n",
        "\n",
        "        consensus_score = sum(weighted_scores) / total_weight\n",
        "        results[label] = consensus_score\n",
        "\n",
        "    thresholds = {\"text\": 0.5, \"table\": 0.5}\n",
        "    predictions = {label: score > thresholds[label] for label, score in results.items()}\n",
        "\n",
        "    return {\n",
        "        \"model\": model_name,\n",
        "        \"predictions\": predictions,\n",
        "        \"scores\": results,\n",
        "        \"thresholds\": thresholds\n",
        "    }"
      ],
      "metadata": {
        "id": "a_tWEkygb3u4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_clip_weighted_v3_1 = evaluate_binary_multilabel_model(\n",
        "    path_folder_test_baseline\n",
        "    ,ground_truth_pu_p01_pp01_multilabel\n",
        "    ,detect_multilabel_clip_weighted_v3_1\n",
        "    ,processor_clip\n",
        "    ,model_clip\n",
        ")\n",
        "\n",
        "print(f\"\\n📊 Résultats CLIP:\")\n",
        "print(f\"F1 Text: {metrics_clip_weighted_v3_1['f1_text']:.3f}\")\n",
        "print(f\"F1 Table: {metrics_clip_weighted_v3_1['f1_table']:.3f}\")\n",
        "print(f\"F1 Macro: {metrics_clip_weighted_v3_1['f1_macro']:.3f}\")\n",
        "print(f\"Hamming loss: {metrics_clip_weighted_v3_1['hamming_loss']:.3f}\")\n",
        "print(f\"Jaccard: {metrics_clip_weighted_v3_1['jaccard_macro']:.3f} & {metrics_clip_weighted_v3_1['jaccard_micro']:.3f}\")\n",
        "print(f\"Jaccard samples :{metrics_clip_weighted_v3_1['jaccard_samples']:.3f}\")\n",
        "print(f\"Jaccard per class: {metrics_clip_weighted_v3_1['jaccard_per_class']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BUT24HObd2Ic",
        "outputId": "aec6f3b0-229e-475e-975f-7465905230de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " p1: ✅ ✅ | Text:0.983 vs Table:0.350\n",
            " p2: ✅ ✅ | Text:0.934 vs Table:0.096\n",
            " p3: ✅ ❌ | Text:0.978 vs Table:0.405\n",
            " p4: ✅ ✅ | Text:0.964 vs Table:0.237\n",
            " p5: ✅ ✅ | Text:0.957 vs Table:0.495\n",
            " p6: ✅ ✅ | Text:0.969 vs Table:0.188\n",
            " p7: ✅ ✅ | Text:0.818 vs Table:0.494\n",
            " p8: ✅ ❌ | Text:0.889 vs Table:0.375\n",
            " p9: ✅ ✅ | Text:0.968 vs Table:0.187\n",
            " p10: ✅ ✅ | Text:0.962 vs Table:0.101\n",
            " p11: ✅ ✅ | Text:0.733 vs Table:0.302\n",
            " p12: ✅ ✅ | Text:0.948 vs Table:0.668\n",
            " p13: ✅ ❌ | Text:0.960 vs Table:0.481\n",
            " p14: ✅ ✅ | Text:0.976 vs Table:0.401\n",
            " p15: ✅ ❌ | Text:0.905 vs Table:0.363\n",
            " p16: ✅ ✅ | Text:0.941 vs Table:0.321\n",
            " p17: ✅ ✅ | Text:0.969 vs Table:0.157\n",
            " p18: ✅ ✅ | Text:0.968 vs Table:0.233\n",
            " p19: ✅ ✅ | Text:0.930 vs Table:0.181\n",
            " p20: ✅ ✅ | Text:0.975 vs Table:0.430\n",
            " p21: ✅ ✅ | Text:0.923 vs Table:0.317\n",
            " p22: ✅ ❌ | Text:0.873 vs Table:0.358\n",
            " p23: ✅ ✅ | Text:0.859 vs Table:0.570\n",
            " p24: ✅ ✅ | Text:0.884 vs Table:0.292\n",
            " p25: ✅ ✅ | Text:0.937 vs Table:0.264\n",
            " p26: ✅ ✅ | Text:0.964 vs Table:0.339\n",
            " p27: ✅ ✅ | Text:0.942 vs Table:0.340\n",
            " p28: ✅ ❌ | Text:0.842 vs Table:0.510\n",
            " p29: ✅ ✅ | Text:0.953 vs Table:0.221\n",
            " p30: ✅ ❌ | Text:0.917 vs Table:0.385\n",
            " p31: ✅ ❌ | Text:0.952 vs Table:0.267\n",
            " p32: ✅ ❌ | Text:0.987 vs Table:0.190\n",
            " p33: ✅ ✅ | Text:0.951 vs Table:0.298\n",
            " p34: ✅ ✅ | Text:0.934 vs Table:0.085\n",
            " p35: ✅ ❌ | Text:0.939 vs Table:0.337\n",
            " p36: ✅ ❌ | Text:0.919 vs Table:0.306\n",
            " p37: ✅ ❌ | Text:0.890 vs Table:0.424\n",
            " p38: ✅ ❌ | Text:0.982 vs Table:0.344\n",
            " p39: ✅ ✅ | Text:0.950 vs Table:0.157\n",
            " p40: ✅ ✅ | Text:0.966 vs Table:0.392\n",
            " p41: ✅ ✅ | Text:0.970 vs Table:0.654\n",
            " p42: ✅ ✅ | Text:0.914 vs Table:0.618\n",
            " p43: ✅ ✅ | Text:0.930 vs Table:0.721\n",
            " p44: ✅ ✅ | Text:0.958 vs Table:0.701\n",
            " p45: ✅ ✅ | Text:0.961 vs Table:0.672\n",
            " p46: ✅ ✅ | Text:0.886 vs Table:0.373\n",
            " p47: ✅ ✅ | Text:0.897 vs Table:0.698\n",
            " p48: ✅ ✅ | Text:0.865 vs Table:0.741\n",
            " p49: ✅ ✅ | Text:0.783 vs Table:0.814\n",
            "\n",
            "📊 Résultats CLIP:\n",
            "F1 Text: 1.000\n",
            "F1 Table: 0.606\n",
            "F1 Macro: 0.803\n",
            "Hamming loss: 0.133\n",
            "Jaccard: 0.717 & 0.819\n",
            "Jaccard samples :0.867\n",
            "Jaccard per class: [1.         0.43478261]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_clip_weighted_v3_1 = evaluate_binary_multilabel_model(\n",
        "    path_folder_test_baseline\n",
        "    ,ground_truth_pu_p01_pp01_multilabel\n",
        "    ,detect_multilabel_clip_weighted_v3_1\n",
        "    ,processor_clip\n",
        "    ,model_clip\n",
        ")\n",
        "\n",
        "print(f\"\\n📊 Résultats CLIP:\")\n",
        "print(f\"F1 Text: {metrics_clip_weighted_v3_1['f1_text']:.3f}\")\n",
        "print(f\"F1 Table: {metrics_clip_weighted_v3_1['f1_table']:.3f}\")\n",
        "print(f\"F1 Macro: {metrics_clip_weighted_v3_1['f1_macro']:.3f}\")\n",
        "print(f\"Hamming loss: {metrics_clip_weighted_v3_1['hamming_loss']:.3f}\")\n",
        "print(f\"Jaccard: {metrics_clip_weighted_v3_1['jaccard_macro']:.3f} & {metrics_clip_weighted_v3_1['jaccard_micro']:.3f}\")\n",
        "print(f\"Jaccard samples :{metrics_clip_weighted_v3_1['jaccard_samples']:.3f}\")\n",
        "print(f\"Jaccard per class: {metrics_clip_weighted_v3_1['jaccard_per_class']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zdG-4ZsncFSh",
        "outputId": "6bc9fad7-cc95-4956-829b-4f5b48180bcf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " p1: ✅ ✅ | Text:0.983 vs Table:0.369\n",
            " p2: ✅ ✅ | Text:0.934 vs Table:0.102\n",
            " p3: ✅ ❌ | Text:0.978 vs Table:0.446\n",
            " p4: ✅ ✅ | Text:0.964 vs Table:0.255\n",
            " p5: ✅ ❌ | Text:0.957 vs Table:0.540\n",
            " p6: ✅ ✅ | Text:0.969 vs Table:0.207\n",
            " p7: ✅ ✅ | Text:0.818 vs Table:0.496\n",
            " p8: ✅ ❌ | Text:0.889 vs Table:0.391\n",
            " p9: ✅ ✅ | Text:0.968 vs Table:0.203\n",
            " p10: ✅ ✅ | Text:0.962 vs Table:0.111\n",
            " p11: ✅ ✅ | Text:0.733 vs Table:0.278\n",
            " p12: ✅ ✅ | Text:0.948 vs Table:0.715\n",
            " p13: ✅ ❌ | Text:0.960 vs Table:0.494\n",
            " p14: ✅ ✅ | Text:0.976 vs Table:0.421\n",
            " p15: ✅ ❌ | Text:0.905 vs Table:0.311\n",
            " p16: ✅ ✅ | Text:0.941 vs Table:0.342\n",
            " p17: ✅ ✅ | Text:0.969 vs Table:0.171\n",
            " p18: ✅ ✅ | Text:0.968 vs Table:0.260\n",
            " p19: ✅ ✅ | Text:0.930 vs Table:0.193\n",
            " p20: ✅ ✅ | Text:0.975 vs Table:0.448\n",
            " p21: ✅ ✅ | Text:0.923 vs Table:0.338\n",
            " p22: ✅ ❌ | Text:0.873 vs Table:0.370\n",
            " p23: ✅ ✅ | Text:0.859 vs Table:0.599\n",
            " p24: ✅ ✅ | Text:0.884 vs Table:0.309\n",
            " p25: ✅ ✅ | Text:0.937 vs Table:0.263\n",
            " p26: ✅ ✅ | Text:0.964 vs Table:0.351\n",
            " p27: ✅ ✅ | Text:0.942 vs Table:0.362\n",
            " p28: ✅ ❌ | Text:0.842 vs Table:0.557\n",
            " p29: ✅ ✅ | Text:0.953 vs Table:0.236\n",
            " p30: ✅ ❌ | Text:0.917 vs Table:0.407\n",
            " p31: ✅ ❌ | Text:0.952 vs Table:0.268\n",
            " p32: ✅ ❌ | Text:0.987 vs Table:0.200\n",
            " p33: ✅ ✅ | Text:0.951 vs Table:0.318\n",
            " p34: ✅ ✅ | Text:0.934 vs Table:0.093\n",
            " p35: ✅ ❌ | Text:0.939 vs Table:0.350\n",
            " p36: ✅ ❌ | Text:0.919 vs Table:0.315\n",
            " p37: ✅ ❌ | Text:0.890 vs Table:0.440\n",
            " p38: ✅ ❌ | Text:0.982 vs Table:0.370\n",
            " p39: ✅ ✅ | Text:0.950 vs Table:0.171\n",
            " p40: ✅ ✅ | Text:0.966 vs Table:0.427\n",
            " p41: ✅ ✅ | Text:0.970 vs Table:0.693\n",
            " p42: ✅ ✅ | Text:0.914 vs Table:0.667\n",
            " p43: ✅ ✅ | Text:0.930 vs Table:0.776\n",
            " p44: ✅ ✅ | Text:0.958 vs Table:0.748\n",
            " p45: ✅ ✅ | Text:0.961 vs Table:0.713\n",
            " p46: ✅ ✅ | Text:0.886 vs Table:0.377\n",
            " p47: ✅ ✅ | Text:0.897 vs Table:0.683\n",
            " p48: ✅ ✅ | Text:0.865 vs Table:0.721\n",
            " p49: ✅ ✅ | Text:0.783 vs Table:0.825\n",
            "\n",
            "📊 Résultats CLIP:\n",
            "F1 Text: 1.000\n",
            "F1 Table: 0.588\n",
            "F1 Macro: 0.794\n",
            "Hamming loss: 0.143\n",
            "Jaccard: 0.708 & 0.808\n",
            "Jaccard samples :0.857\n",
            "Jaccard per class: [1.         0.41666667]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Grid search CV sur les 3 prompts de consensus v3"
      ],
      "metadata": {
        "id": "7hZvZTS-e22g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools"
      ],
      "metadata": {
        "id": "3_baV2tJfByf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_multilabel_clip_grid_v3(image_path, processor, model, weights):\n",
        "    \"\"\"Version grid search allégée - consensus v3 uniquement\"\"\"\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "    prompts_variants = {\n",
        "        \"text\": [\n",
        "            (\"document with printed text and readable content\", 1.0)\n",
        "        ],\n",
        "        \"table\": [\n",
        "            (\"administrative form with budget tables and financial data\", weights[0])\n",
        "            ,(\"document with tabular information and structured data\", weights[1])\n",
        "            ,(\"document containing structured information in table format\", weights[2])\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    results = {}\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    for label, prompts_list in prompts_variants.items():\n",
        "        weighted_scores = []\n",
        "        total_weight = 0\n",
        "\n",
        "        for prompt, weight in prompts_list:\n",
        "            binary_prompts = [prompt, f\"document without {label}\"]\n",
        "            inputs = processor(text=binary_prompts, images=image,\n",
        "                             return_tensors=\"pt\", padding=True)\n",
        "            inputs = {key: value.to(device) for key, value in inputs.items()}\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = model(**inputs)\n",
        "                probs = outputs.logits_per_image.softmax(dim=1)\n",
        "\n",
        "            score = float(probs[0][0])\n",
        "            weighted_scores.append(score * weight)\n",
        "            total_weight += weight\n",
        "\n",
        "        consensus_score = sum(weighted_scores) / total_weight\n",
        "        results[label] = consensus_score\n",
        "\n",
        "    thresholds = {\"text\": 0.5, \"table\": 0.5}\n",
        "    predictions = {label: score > thresholds[label] for label, score in results.items()}\n",
        "\n",
        "    return predictions"
      ],
      "metadata": {
        "id": "_Gs8gXHue8cG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lightweight_grid_search_v3(path_dict, ground_truth, processor, model):\n",
        "    \"\"\"Grid search ultra léger - focus sur les gains\"\"\"\n",
        "\n",
        "    print(\"🔍 GRID SEARCH LÉGER v3 - Optimisation 3 poids\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # Ranges très focalisés autour des valeurs actuelles\n",
        "    ranges = {\n",
        "        'admin_form': [1.2, 1.3, 1.4],        # ±0.1 autour de 1.3\n",
        "        'tabular_info': [1.7, 1.8, 1.9],      # ±0.1 autour de 1.8\n",
        "        'table_format': [0.9, 1.0, 1.1]       # ±0.1 autour de 1.0\n",
        "    }\n",
        "\n",
        "    # 27 combinaisons (3×3×3)\n",
        "    combinations = list(itertools.product(\n",
        "        ranges['admin_form'],\n",
        "        ranges['tabular_info'],\n",
        "        ranges['table_format']\n",
        "    ))\n",
        "\n",
        "    print(f\"📊 {len(combinations)} combinaisons à tester\")\n",
        "\n",
        "    best_f1_table = 0\n",
        "    best_weights = None\n",
        "    best_results = None\n",
        "\n",
        "    current_baseline = [1.3, 1.8, 1.0]  # Poids actuels\n",
        "\n",
        "    # Test baseline d'abord\n",
        "    print(f\"\\n🎯 BASELINE: {current_baseline}\")\n",
        "    y_true_table, y_pred_table = [], []\n",
        "\n",
        "    for page, image_path in path_dict.items():\n",
        "        pred = detect_multilabel_clip_grid_v3(image_path, processor, model, current_baseline)\n",
        "        y_true_table.append(ground_truth[page]['table'])\n",
        "        y_pred_table.append(pred['table'])\n",
        "\n",
        "    baseline_f1 = f1_score(y_true_table, y_pred_table)\n",
        "    print(f\"   F1 Table baseline: {baseline_f1:.3f}\")\n",
        "\n",
        "    improvements = []\n",
        "\n",
        "    for i, weights in enumerate(combinations):\n",
        "        if list(weights) == current_baseline:\n",
        "            continue  # Skip baseline déjà testé\n",
        "\n",
        "        print(f\"\\r🧪 Test {i+1}/{len(combinations)}: {weights}\", end=\"\")\n",
        "\n",
        "        try:\n",
        "            y_true_table, y_pred_table = [], []\n",
        "\n",
        "            for page, image_path in path_dict.items():\n",
        "                pred = detect_multilabel_clip_grid_v3(image_path, processor, model, weights)\n",
        "                y_true_table.append(ground_truth[page]['table'])\n",
        "                y_pred_table.append(pred['table'])\n",
        "\n",
        "            f1_table = f1_score(y_true_table, y_pred_table)\n",
        "\n",
        "            if f1_table > baseline_f1:\n",
        "                improvement = f1_table - baseline_f1\n",
        "                improvements.append((weights, f1_table, improvement))\n",
        "\n",
        "            if f1_table > best_f1_table:\n",
        "                best_f1_table = f1_table\n",
        "                best_weights = weights\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\n❌ Erreur: {e}\")\n",
        "\n",
        "    print(f\"\\n\\n🏆 RÉSULTATS:\")\n",
        "    print(\"=\"*40)\n",
        "\n",
        "    if improvements:\n",
        "        print(f\"✅ {len(improvements)} améliorations trouvées!\")\n",
        "\n",
        "        # Top 5 améliorations\n",
        "        improvements.sort(key=lambda x: x[2], reverse=True)\n",
        "        print(f\"\\n🔥 TOP améliorations:\")\n",
        "        for i, (weights, f1, gain) in enumerate(improvements[:5]):\n",
        "            print(f\"   {i+1}. {weights} → F1: {f1:.3f} (+{gain:+.3f})\")\n",
        "\n",
        "        print(f\"\\n🎯 MEILLEUR:\")\n",
        "        best = improvements[0]\n",
        "        print(f\"   Poids: {best[0]}\")\n",
        "        print(f\"   F1 Table: {best[1]:.3f}\")\n",
        "        print(f\"   Gain: +{best[2]:.3f} ({best[2]/baseline_f1*100:+.1f}%)\")\n",
        "\n",
        "    else:\n",
        "        print(f\"❌ Aucune amélioration trouvée\")\n",
        "        print(f\"   Baseline {current_baseline} reste optimal: {baseline_f1:.3f}\")\n",
        "\n",
        "    return {\n",
        "        'baseline_f1': baseline_f1,\n",
        "        'baseline_weights': current_baseline,\n",
        "        'best_weights': best_weights,\n",
        "        'best_f1': best_f1_table,\n",
        "        'improvements': improvements\n",
        "    }"
      ],
      "metadata": {
        "id": "XijRl3rpfIBU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def quick_test_weights(path_dict, ground_truth, processor, model, test_weights):\n",
        "    \"\"\"Test rapide d'un set de poids spécifique\"\"\"\n",
        "\n",
        "    print(f\"⚡ TEST RAPIDE: {test_weights}\")\n",
        "\n",
        "    y_true_table, y_pred_table = [], []\n",
        "\n",
        "    for page, image_path in path_dict.items():\n",
        "        pred = detect_multilabel_clip_grid_v3(image_path, processor, model, test_weights)\n",
        "        y_true_table.append(ground_truth[page]['table'])\n",
        "        y_pred_table.append(pred['table'])\n",
        "\n",
        "    f1_table = f1_score(y_true_table, y_pred_table)\n",
        "    print(f\"   F1 Table: {f1_table:.3f}\")\n",
        "\n",
        "    return f1_table"
      ],
      "metadata": {
        "id": "6bnK7HXXfNNQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = lightweight_grid_search_v3(path_pu_p01_pp01\n",
        "                                     ,ground_truth_pu_p01_pp01_multilabel\n",
        "                                     ,processor_clip\n",
        "                                     ,model_clip)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dCS6_Z7ZfZYL",
        "outputId": "ff610545-ea06-440b-f4b5-db6d290a51d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔍 GRID SEARCH LÉGER v3 - Optimisation 3 poids\n",
            "==================================================\n",
            "📊 27 combinaisons à tester\n",
            "\n",
            "🎯 BASELINE: [1.3, 1.8, 1.0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n",
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n",
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"/tmp/ipython-input-34-2777842423.py\", line 1, in <cell line: 0>\n",
            "    results = lightweight_grid_search_v3(path_pu_p01_pp01\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-32-2914503883.py\", line 34, in lightweight_grid_search_v3\n",
            "    pred = detect_multilabel_clip_grid_v3(image_path, processor, model, current_baseline)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-31-3852009679.py\", line 30, in detect_multilabel_clip_grid_v3\n",
            "    outputs = model(**inputs)\n",
            "              ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py\", line 943, in wrapper\n",
            "    output = func(self, *args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py\", line 1015, in forward\n",
            "    text_outputs: BaseModelOutputWithPooling = self.text_model(\n",
            "                                               ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py\", line 943, in wrapper\n",
            "    output = func(self, *args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py\", line 615, in forward\n",
            "    hidden_states = self.embeddings(input_ids=input_ids, position_ids=position_ids)\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py\", line 247, in forward\n",
            "    inputs_embeds = self.token_embedding(input_ids)\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/sparse.py\", line 190, in forward\n",
            "    return F.embedding(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\", line 2551, in embedding\n",
            "    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/inspect.py\", line 1739, in getinnerframes\n",
            "    traceback_info = getframeinfo(tb, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/inspect.py\", line 1684, in getframeinfo\n",
            "    filename = getsourcefile(frame) or getfile(frame)\n",
            "               ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/inspect.py\", line 948, in getsourcefile\n",
            "    module = getmodule(object, filename)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/inspect.py\", line 997, in getmodule\n",
            "    os.path.realpath(f)] = module.__name__\n",
            "    ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"<frozen posixpath>\", line 416, in realpath\n",
            "  File \"<frozen posixpath>\", line 460, in _joinrealpath\n",
            "KeyboardInterrupt\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"/tmp/ipython-input-34-2777842423.py\", line 1, in <cell line: 0>\n",
            "    results = lightweight_grid_search_v3(path_pu_p01_pp01\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-32-2914503883.py\", line 34, in lightweight_grid_search_v3\n",
            "    pred = detect_multilabel_clip_grid_v3(image_path, processor, model, current_baseline)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-31-3852009679.py\", line 30, in detect_multilabel_clip_grid_v3\n",
            "    outputs = model(**inputs)\n",
            "              ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py\", line 943, in wrapper\n",
            "    output = func(self, *args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py\", line 1015, in forward\n",
            "    text_outputs: BaseModelOutputWithPooling = self.text_model(\n",
            "                                               ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py\", line 943, in wrapper\n",
            "    output = func(self, *args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py\", line 615, in forward\n",
            "    hidden_states = self.embeddings(input_ids=input_ids, position_ids=position_ids)\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py\", line 247, in forward\n",
            "    inputs_embeds = self.token_embedding(input_ids)\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/sparse.py\", line 190, in forward\n",
            "    return F.embedding(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\", line 2551, in embedding\n",
            "    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3575, in run_code\n",
            "    self.showtraceback(running_compiled_code=True)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2101, in showtraceback\n",
            "    stb = self.InteractiveTB.structured_traceback(etype,\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 1367, in structured_traceback\n",
            "    return FormattedTB.structured_traceback(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 1267, in structured_traceback\n",
            "    return VerboseTB.structured_traceback(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 1124, in structured_traceback\n",
            "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 1082, in format_exception_as_a_whole\n",
            "    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n",
            "                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 382, in find_recursion\n",
            "    return len(records), 0\n",
            "           ^^^^^^^^^^^^\n",
            "TypeError: object of type 'NoneType' has no len()\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/inspect.py\", line 1739, in getinnerframes\n",
            "    traceback_info = getframeinfo(tb, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/inspect.py\", line 1684, in getframeinfo\n",
            "    filename = getsourcefile(frame) or getfile(frame)\n",
            "               ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/inspect.py\", line 939, in getsourcefile\n",
            "    if any(filename.endswith(s) for s in all_bytecode_suffixes):\n",
            "       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/inspect.py\", line 939, in <genexpr>\n",
            "    if any(filename.endswith(s) for s in all_bytecode_suffixes):\n",
            "    \n",
            "KeyboardInterrupt\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"/tmp/ipython-input-34-2777842423.py\", line 1, in <cell line: 0>\n",
            "    results = lightweight_grid_search_v3(path_pu_p01_pp01\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-32-2914503883.py\", line 34, in lightweight_grid_search_v3\n",
            "    pred = detect_multilabel_clip_grid_v3(image_path, processor, model, current_baseline)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-31-3852009679.py\", line 30, in detect_multilabel_clip_grid_v3\n",
            "    outputs = model(**inputs)\n",
            "              ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py\", line 943, in wrapper\n",
            "    output = func(self, *args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py\", line 1015, in forward\n",
            "    text_outputs: BaseModelOutputWithPooling = self.text_model(\n",
            "                                               ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py\", line 943, in wrapper\n",
            "    output = func(self, *args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py\", line 615, in forward\n",
            "    hidden_states = self.embeddings(input_ids=input_ids, position_ids=position_ids)\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/clip/modeling_clip.py\", line 247, in forward\n",
            "    inputs_embeds = self.token_embedding(input_ids)\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/sparse.py\", line 190, in forward\n",
            "    return F.embedding(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\", line 2551, in embedding\n",
            "    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3575, in run_code\n",
            "    self.showtraceback(running_compiled_code=True)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2101, in showtraceback\n",
            "    stb = self.InteractiveTB.structured_traceback(etype,\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 1367, in structured_traceback\n",
            "    return FormattedTB.structured_traceback(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 1267, in structured_traceback\n",
            "    return VerboseTB.structured_traceback(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 1124, in structured_traceback\n",
            "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 1082, in format_exception_as_a_whole\n",
            "    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n",
            "                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 382, in find_recursion\n",
            "    return len(records), 0\n",
            "           ^^^^^^^^^^^^\n",
            "TypeError: object of type 'NoneType' has no len()\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3492, in run_ast_nodes\n",
            "    self.showtraceback()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2101, in showtraceback\n",
            "    stb = self.InteractiveTB.structured_traceback(etype,\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 1367, in structured_traceback\n",
            "    return FormattedTB.structured_traceback(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 1267, in structured_traceback\n",
            "    return VerboseTB.structured_traceback(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 1142, in structured_traceback\n",
            "    formatted_exceptions += self.format_exception_as_a_whole(etype, evalue, etb, lines_of_context,\n",
            "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 1082, in format_exception_as_a_whole\n",
            "    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n",
            "                                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 382, in find_recursion\n",
            "    return len(records), 0\n",
            "           ^^^^^^^^^^^^\n",
            "TypeError: object of type 'NoneType' has no len()\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/inspect.py\", line 1739, in getinnerframes\n",
            "    traceback_info = getframeinfo(tb, context)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/inspect.py\", line 1684, in getframeinfo\n",
            "    filename = getsourcefile(frame) or getfile(frame)\n",
            "               ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/inspect.py\", line 948, in getsourcefile\n",
            "    module = getmodule(object, filename)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/inspect.py\", line 988, in getmodule\n",
            "    if ismodule(module) and hasattr(module, '__file__'):\n",
            "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test after Grid\n",
        "\n",
        "Test des 5 Top améliorations"
      ],
      "metadata": {
        "id": "Lidbpx2KmZft"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_multilabel_clip_grid_v3_1(image_path, processor, model, model_name='CLIP v3.1 Weighted'):\n",
        "    \"\"\"CLIP consensus v3.1 avec poids optimisés basés sur l'analyse\"\"\"\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "    # Poids ajustés basés sur les performances observées\n",
        "    prompts_variants = {\n",
        "        \"text\": [\n",
        "            (\"document with printed text and readable content\", 1.0)\n",
        "        ],\n",
        "        \"table\": [\n",
        "            # Prompts v3 maintenus avec poids élevés (prouvés efficaces)\n",
        "            (\"administrative form with budget tables and financial data\", 1.2)     # ↑ Excellent sur Excel\n",
        "            ,(\"document with tabular information and structured data\", 1.7)        # ↓ Très bon généraliste\n",
        "            ,(\"document containing structured information in table format\", 1.0)   # ↑ Support solide\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    results = {}\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    for label, prompts_list in prompts_variants.items():\n",
        "        weighted_scores = []\n",
        "        total_weight = 0\n",
        "\n",
        "        for prompt, weight in prompts_list:\n",
        "            binary_prompts = [prompt, f\"document without {label}\"]\n",
        "\n",
        "            inputs = processor(text=binary_prompts\n",
        "                             ,images=image\n",
        "                             ,return_tensors=\"pt\"\n",
        "                             ,padding=True)\n",
        "            inputs = {key: value.to(device) for key, value in inputs.items()}\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = model(**inputs)\n",
        "                probs = outputs.logits_per_image.softmax(dim=1)\n",
        "\n",
        "            score = float(probs[0][0])\n",
        "            weighted_scores.append(score * weight)\n",
        "            total_weight += weight\n",
        "\n",
        "        consensus_score = sum(weighted_scores) / total_weight\n",
        "        results[label] = consensus_score\n",
        "\n",
        "    thresholds = {\"text\": 0.5, \"table\": 0.5}\n",
        "    predictions = {label: score > thresholds[label] for label, score in results.items()}\n",
        "\n",
        "    return {\n",
        "        \"model\": model_name,\n",
        "        \"predictions\": predictions,\n",
        "        \"scores\": results,\n",
        "        \"thresholds\": thresholds\n",
        "    }"
      ],
      "metadata": {
        "id": "-eW1CMc7mPDo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_clip_grid_v3_1 = evaluate_binary_multilabel_model(\n",
        "    path_folder_test_baseline\n",
        "    ,ground_truth_pu_p01_pp01_multilabel\n",
        "    ,detect_multilabel_clip_grid_v3_1\n",
        "    ,processor_clip\n",
        "    ,model_clip\n",
        ")\n",
        "\n",
        "print(f\"\\n📊 Résultats CLIP:\")\n",
        "print(f\"F1 Text: {metrics_clip_grid_v3_1['f1_text']:.3f}\")\n",
        "print(f\"F1 Table: {metrics_clip_grid_v3_1['f1_table']:.3f}\")\n",
        "print(f\"F1 Macro: {metrics_clip_grid_v3_1['f1_macro']:.3f}\")\n",
        "print(f\"Hamming loss: {metrics_clip_grid_v3_1['hamming_loss']:.3f}\")\n",
        "print(f\"Jaccard: {metrics_clip_grid_v3_1['jaccard_macro']:.3f} & {metrics_clip_grid_v3_1['jaccard_micro']:.3f}\")\n",
        "print(f\"Jaccard samples :{metrics_clip_grid_v3_1['jaccard_samples']:.3f}\")\n",
        "print(f\"Jaccard per class: {metrics_clip_grid_v3_1['jaccard_per_class']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXeimqy8myla",
        "outputId": "39f4dfe2-9158-4275-ede4-96e62a1865c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " p1: ✅ ✅ | Text:0.983 vs Table:0.430\n",
            " p2: ✅ ✅ | Text:0.934 vs Table:0.112\n",
            " p3: ✅ ✅ | Text:0.978 vs Table:0.511\n",
            " p4: ✅ ✅ | Text:0.964 vs Table:0.282\n",
            " p5: ✅ ❌ | Text:0.957 vs Table:0.611\n",
            " p6: ✅ ✅ | Text:0.969 vs Table:0.234\n",
            " p7: ✅ ✅ | Text:0.818 vs Table:0.498\n",
            " p8: ✅ ❌ | Text:0.889 vs Table:0.439\n",
            " p9: ✅ ✅ | Text:0.968 vs Table:0.228\n",
            " p10: ✅ ✅ | Text:0.962 vs Table:0.122\n",
            " p11: ✅ ✅ | Text:0.733 vs Table:0.219\n",
            " p12: ✅ ✅ | Text:0.948 vs Table:0.801\n",
            " p13: ✅ ✅ | Text:0.960 vs Table:0.512\n",
            " p14: ✅ ✅ | Text:0.976 vs Table:0.460\n",
            " p15: ✅ ❌ | Text:0.905 vs Table:0.223\n",
            " p16: ✅ ✅ | Text:0.941 vs Table:0.368\n",
            " p17: ✅ ✅ | Text:0.969 vs Table:0.191\n",
            " p18: ✅ ✅ | Text:0.968 vs Table:0.292\n",
            " p19: ✅ ✅ | Text:0.930 vs Table:0.213\n",
            " p20: ✅ ✅ | Text:0.975 vs Table:0.488\n",
            " p21: ✅ ✅ | Text:0.923 vs Table:0.379\n",
            " p22: ✅ ❌ | Text:0.873 vs Table:0.419\n",
            " p23: ✅ ✅ | Text:0.859 vs Table:0.676\n",
            " p24: ✅ ✅ | Text:0.884 vs Table:0.341\n",
            " p25: ✅ ✅ | Text:0.937 vs Table:0.277\n",
            " p26: ✅ ✅ | Text:0.964 vs Table:0.389\n",
            " p27: ✅ ✅ | Text:0.942 vs Table:0.410\n",
            " p28: ✅ ❌ | Text:0.842 vs Table:0.626\n",
            " p29: ✅ ✅ | Text:0.953 vs Table:0.246\n",
            " p30: ✅ ❌ | Text:0.917 vs Table:0.453\n",
            " p31: ✅ ❌ | Text:0.952 vs Table:0.288\n",
            " p32: ✅ ❌ | Text:0.987 vs Table:0.207\n",
            " p33: ✅ ✅ | Text:0.951 vs Table:0.353\n",
            " p34: ✅ ✅ | Text:0.934 vs Table:0.103\n",
            " p35: ✅ ❌ | Text:0.939 vs Table:0.387\n",
            " p36: ✅ ❌ | Text:0.919 vs Table:0.346\n",
            " p37: ✅ ❌ | Text:0.890 vs Table:0.483\n",
            " p38: ✅ ❌ | Text:0.982 vs Table:0.416\n",
            " p39: ✅ ✅ | Text:0.950 vs Table:0.190\n",
            " p40: ✅ ✅ | Text:0.966 vs Table:0.477\n",
            " p41: ✅ ✅ | Text:0.970 vs Table:0.762\n",
            " p42: ✅ ✅ | Text:0.914 vs Table:0.757\n",
            " p43: ✅ ✅ | Text:0.930 vs Table:0.878\n",
            " p44: ✅ ✅ | Text:0.958 vs Table:0.834\n",
            " p45: ✅ ✅ | Text:0.961 vs Table:0.794\n",
            " p46: ✅ ✅ | Text:0.886 vs Table:0.374\n",
            " p47: ✅ ✅ | Text:0.897 vs Table:0.671\n",
            " p48: ✅ ✅ | Text:0.865 vs Table:0.700\n",
            " p49: ✅ ✅ | Text:0.783 vs Table:0.845\n",
            "\n",
            "📊 Résultats CLIP:\n",
            "F1 Text: 1.000\n",
            "F1 Table: 0.667\n",
            "F1 Macro: 0.833\n",
            "Hamming loss: 0.122\n",
            "Jaccard: 0.750 & 0.836\n",
            "Jaccard samples :0.878\n",
            "Jaccard per class: [1.  0.5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "perf_grid_v3_1 = measure_performance_v2(\n",
        "    evaluate_binary_multilabel_model,\n",
        "    path_folder_test_baseline,\n",
        "    ground_truth_pu_p01_pp01_multilabel,\n",
        "    detect_multilabel_clip_grid_v3_1,\n",
        "    processor_clip,\n",
        "    model_clip\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SIrsrDNHwTnf",
        "outputId": "1ca8b74d-0097-4cc4-d64a-8c8c696ec3a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " p1: ✅ ❌ | Text:0.983 vs Table:0.613\n",
            " p2: ✅ ✅ | Text:0.934 vs Table:0.095\n",
            " p3: ✅ ✅ | Text:0.978 vs Table:0.550\n",
            " p4: ✅ ✅ | Text:0.964 vs Table:0.200\n",
            " p5: ✅ ❌ | Text:0.957 vs Table:0.604\n",
            " p6: ✅ ✅ | Text:0.969 vs Table:0.211\n",
            " p7: ✅ ❌ | Text:0.818 vs Table:0.602\n",
            " p8: ✅ ❌ | Text:0.889 vs Table:0.440\n",
            " p9: ✅ ✅ | Text:0.968 vs Table:0.224\n",
            " p10: ✅ ✅ | Text:0.962 vs Table:0.088\n",
            " p11: ✅ ✅ | Text:0.733 vs Table:0.233\n",
            " p12: ✅ ✅ | Text:0.948 vs Table:0.814\n",
            " p13: ✅ ✅ | Text:0.960 vs Table:0.536\n",
            " p14: ✅ ✅ | Text:0.976 vs Table:0.468\n",
            " p15: ✅ ❌ | Text:0.905 vs Table:0.294\n",
            " p16: ✅ ✅ | Text:0.941 vs Table:0.448\n",
            " p17: ✅ ✅ | Text:0.969 vs Table:0.162\n",
            " p18: ✅ ✅ | Text:0.968 vs Table:0.203\n",
            " p19: ✅ ✅ | Text:0.930 vs Table:0.138\n",
            " p20: ✅ ❌ | Text:0.975 vs Table:0.515\n",
            " p21: ✅ ✅ | Text:0.923 vs Table:0.340\n",
            " p22: ✅ ❌ | Text:0.873 vs Table:0.475\n",
            " p23: ✅ ✅ | Text:0.859 vs Table:0.703\n",
            " p24: ✅ ✅ | Text:0.884 vs Table:0.326\n",
            " p25: ✅ ✅ | Text:0.937 vs Table:0.340\n",
            " p26: ✅ ✅ | Text:0.964 vs Table:0.477\n",
            " p27: ✅ ✅ | Text:0.942 vs Table:0.494\n",
            " p28: ✅ ❌ | Text:0.842 vs Table:0.588\n",
            " p29: ✅ ✅ | Text:0.953 vs Table:0.274\n",
            " p30: ✅ ❌ | Text:0.917 vs Table:0.372\n",
            " p31: ✅ ❌ | Text:0.952 vs Table:0.336\n",
            " p32: ✅ ❌ | Text:0.987 vs Table:0.189\n",
            " p33: ✅ ✅ | Text:0.951 vs Table:0.291\n",
            " p34: ✅ ✅ | Text:0.934 vs Table:0.085\n",
            " p35: ✅ ❌ | Text:0.939 vs Table:0.392\n",
            " p36: ✅ ❌ | Text:0.919 vs Table:0.370\n",
            " p37: ✅ ❌ | Text:0.890 vs Table:0.484\n",
            " p38: ✅ ❌ | Text:0.982 vs Table:0.401\n",
            " p39: ✅ ✅ | Text:0.950 vs Table:0.176\n",
            " p40: ✅ ✅ | Text:0.966 vs Table:0.361\n",
            " p41: ✅ ✅ | Text:0.970 vs Table:0.797\n",
            " p42: ✅ ✅ | Text:0.914 vs Table:0.809\n",
            " p43: ✅ ✅ | Text:0.930 vs Table:0.916\n",
            " p44: ✅ ✅ | Text:0.958 vs Table:0.862\n",
            " p45: ✅ ✅ | Text:0.961 vs Table:0.892\n",
            " p46: ✅ ✅ | Text:0.886 vs Table:0.433\n",
            " p47: ✅ ✅ | Text:0.897 vs Table:0.849\n",
            " p48: ✅ ✅ | Text:0.865 vs Table:0.891\n",
            " p49: ✅ ✅ | Text:0.783 vs Table:0.890\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"⏱️ Temps : {perf_grid_v3_1['time_seconds']:.2f} sec\")\n",
        "print(f\"🧠 RAM CPU delta : {perf_grid_v3_1['cpu_ram_mb_delta']:.2f} MB\")\n",
        "print(f\"📈 RAM CPU pic : {perf_grid_v3_1['cpu_ram_peak_mb']:.2f} MB\")\n",
        "\n",
        "if perf_grid_v3_1['gpu_vram_peak_mb'] is not None:\n",
        "    print(f\"🚀 VRAM GPU pic : {perf_grid_v3_1['gpu_vram_peak_mb']:.2f} MB\")\n",
        "else:\n",
        "    print(\"⚠️ GPU CUDA non disponible\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VnR7OocMxbUQ",
        "outputId": "d9e7230e-b380-4781-bd7d-0450a843e57d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⏱️ Temps : 39.69 sec\n",
            "🧠 RAM CPU delta : 9.54 MB\n",
            "📈 RAM CPU pic : 4.16 MB\n",
            "🚀 VRAM GPU pic : 0.00 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_multilabel_clip_grid_v3_2(image_path, processor, model, model_name='CLIP v3.2 Weighted'):\n",
        "    \"\"\"CLIP consensus v3.1 avec poids optimisés basés sur l'analyse\"\"\"\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "    # Poids ajustés basés sur les performances observées\n",
        "    prompts_variants = {\n",
        "        \"text\": [\n",
        "            (\"document with printed text and readable content\", 1.0)\n",
        "        ],\n",
        "        \"table\": [\n",
        "            # Prompts v3 maintenus avec poids élevés (prouvés efficaces)\n",
        "            (\"administrative form with budget tables and financial data\", 1.2)     # ↑ Excellent sur Excel\n",
        "            ,(\"document with tabular information and structured data\", 1.7)        # ↓ Très bon généraliste\n",
        "            ,(\"document containing structured information in table format\", 1.1)   # ↑ Support solide\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    results = {}\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    for label, prompts_list in prompts_variants.items():\n",
        "        weighted_scores = []\n",
        "        total_weight = 0\n",
        "\n",
        "        for prompt, weight in prompts_list:\n",
        "            binary_prompts = [prompt, f\"document without {label}\"]\n",
        "\n",
        "            inputs = processor(text=binary_prompts\n",
        "                             ,images=image\n",
        "                             ,return_tensors=\"pt\"\n",
        "                             ,padding=True)\n",
        "            inputs = {key: value.to(device) for key, value in inputs.items()}\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = model(**inputs)\n",
        "                probs = outputs.logits_per_image.softmax(dim=1)\n",
        "\n",
        "            score = float(probs[0][0])\n",
        "            weighted_scores.append(score * weight)\n",
        "            total_weight += weight\n",
        "\n",
        "        consensus_score = sum(weighted_scores) / total_weight\n",
        "        results[label] = consensus_score\n",
        "\n",
        "    thresholds = {\"text\": 0.5, \"table\": 0.5}\n",
        "    predictions = {label: score > thresholds[label] for label, score in results.items()}\n",
        "\n",
        "    return {\n",
        "        \"model\": model_name,\n",
        "        \"predictions\": predictions,\n",
        "        \"scores\": results,\n",
        "        \"thresholds\": thresholds\n",
        "    }"
      ],
      "metadata": {
        "id": "GygJvpato8B_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_clip_grid_v3_2 = evaluate_binary_multilabel_model(\n",
        "    path_folder_test_baseline\n",
        "    ,ground_truth_pu_p01_pp01_multilabel\n",
        "    ,detect_multilabel_clip_grid_v3_2\n",
        "    ,processor_clip\n",
        "    ,model_clip\n",
        ")\n",
        "\n",
        "print(f\"\\n📊 Résultats CLIP:\")\n",
        "print(f\"F1 Text: {metrics_clip_grid_v3_2['f1_text']:.3f}\")\n",
        "print(f\"F1 Table: {metrics_clip_grid_v3_2['f1_table']:.3f}\")\n",
        "print(f\"F1 Macro: {metrics_clip_grid_v3_2['f1_macro']:.3f}\")\n",
        "print(f\"Hamming loss: {metrics_clip_grid_v3_2['hamming_loss']:.3f}\")\n",
        "print(f\"Jaccard: {metrics_clip_grid_v3_2['jaccard_macro']:.3f} & {metrics_clip_grid_v3_2['jaccard_micro']:.3f}\")\n",
        "print(f\"Jaccard samples :{metrics_clip_grid_v3_2['jaccard_samples']:.3f}\")\n",
        "print(f\"Jaccard per class: {metrics_clip_grid_v3_2['jaccard_per_class']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BDqXJiGupAs3",
        "outputId": "d2bd28e0-2f1e-4fc0-aa67-c79553ca6b92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " p1: ✅ ✅ | Text:0.983 vs Table:0.420\n",
            " p2: ✅ ✅ | Text:0.934 vs Table:0.113\n",
            " p3: ✅ ✅ | Text:0.978 vs Table:0.507\n",
            " p4: ✅ ✅ | Text:0.964 vs Table:0.288\n",
            " p5: ✅ ❌ | Text:0.957 vs Table:0.610\n",
            " p6: ✅ ✅ | Text:0.969 vs Table:0.235\n",
            " p7: ✅ ✅ | Text:0.818 vs Table:0.489\n",
            " p8: ✅ ❌ | Text:0.889 vs Table:0.442\n",
            " p9: ✅ ✅ | Text:0.968 vs Table:0.228\n",
            " p10: ✅ ✅ | Text:0.962 vs Table:0.124\n",
            " p11: ✅ ✅ | Text:0.733 vs Table:0.214\n",
            " p12: ✅ ✅ | Text:0.948 vs Table:0.800\n",
            " p13: ✅ ✅ | Text:0.960 vs Table:0.508\n",
            " p14: ✅ ✅ | Text:0.976 vs Table:0.460\n",
            " p15: ✅ ❌ | Text:0.905 vs Table:0.218\n",
            " p16: ✅ ✅ | Text:0.941 vs Table:0.359\n",
            " p17: ✅ ✅ | Text:0.969 vs Table:0.193\n",
            " p18: ✅ ✅ | Text:0.968 vs Table:0.296\n",
            " p19: ✅ ✅ | Text:0.930 vs Table:0.219\n",
            " p20: ✅ ✅ | Text:0.975 vs Table:0.487\n",
            " p21: ✅ ✅ | Text:0.923 vs Table:0.383\n",
            " p22: ✅ ❌ | Text:0.873 vs Table:0.419\n",
            " p23: ✅ ✅ | Text:0.859 vs Table:0.678\n",
            " p24: ✅ ✅ | Text:0.884 vs Table:0.343\n",
            " p25: ✅ ✅ | Text:0.937 vs Table:0.275\n",
            " p26: ✅ ✅ | Text:0.964 vs Table:0.385\n",
            " p27: ✅ ✅ | Text:0.942 vs Table:0.405\n",
            " p28: ✅ ❌ | Text:0.842 vs Table:0.627\n",
            " p29: ✅ ✅ | Text:0.953 vs Table:0.241\n",
            " p30: ✅ ❌ | Text:0.917 vs Table:0.461\n",
            " p31: ✅ ❌ | Text:0.952 vs Table:0.288\n",
            " p32: ✅ ❌ | Text:0.987 vs Table:0.207\n",
            " p33: ✅ ✅ | Text:0.951 vs Table:0.358\n",
            " p34: ✅ ✅ | Text:0.934 vs Table:0.104\n",
            " p35: ✅ ❌ | Text:0.939 vs Table:0.390\n",
            " p36: ✅ ❌ | Text:0.919 vs Table:0.346\n",
            " p37: ✅ ❌ | Text:0.890 vs Table:0.486\n",
            " p38: ✅ ❌ | Text:0.982 vs Table:0.417\n",
            " p39: ✅ ✅ | Text:0.950 vs Table:0.190\n",
            " p40: ✅ ✅ | Text:0.966 vs Table:0.485\n",
            " p41: ✅ ✅ | Text:0.970 vs Table:0.759\n",
            " p42: ✅ ✅ | Text:0.914 vs Table:0.754\n",
            " p43: ✅ ✅ | Text:0.930 vs Table:0.876\n",
            " p44: ✅ ✅ | Text:0.958 vs Table:0.832\n",
            " p45: ✅ ✅ | Text:0.961 vs Table:0.788\n",
            " p46: ✅ ✅ | Text:0.886 vs Table:0.368\n",
            " p47: ✅ ✅ | Text:0.897 vs Table:0.659\n",
            " p48: ✅ ✅ | Text:0.865 vs Table:0.687\n",
            " p49: ✅ ✅ | Text:0.783 vs Table:0.841\n",
            "\n",
            "📊 Résultats CLIP:\n",
            "F1 Text: 1.000\n",
            "F1 Table: 0.667\n",
            "F1 Macro: 0.833\n",
            "Hamming loss: 0.122\n",
            "Jaccard: 0.750 & 0.836\n",
            "Jaccard samples :0.878\n",
            "Jaccard per class: [1.  0.5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_multilabel_clip_grid_v3_3(image_path, processor, model, model_name='CLIP v3.3 Weighted'):\n",
        "    \"\"\"CLIP consensus v3.1 avec poids optimisés basés sur l'analyse\"\"\"\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "    # Poids ajustés basés sur les performances observées\n",
        "    prompts_variants = {\n",
        "        \"text\": [\n",
        "            (\"document with printed text and readable content\", 1.0)\n",
        "        ],\n",
        "        \"table\": [\n",
        "            # Prompts v3 maintenus avec poids élevés (prouvés efficaces)\n",
        "            (\"administrative form with budget tables and financial data\", 1.2)     # ↑ Excellent sur Excel\n",
        "            ,(\"document with tabular information and structured data\", 1.8)        # ↓ Très bon généraliste\n",
        "            ,(\"document containing structured information in table format\", 1.1)   # ↑ Support solide\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    results = {}\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    for label, prompts_list in prompts_variants.items():\n",
        "        weighted_scores = []\n",
        "        total_weight = 0\n",
        "\n",
        "        for prompt, weight in prompts_list:\n",
        "            binary_prompts = [prompt, f\"document without {label}\"]\n",
        "\n",
        "            inputs = processor(text=binary_prompts\n",
        "                             ,images=image\n",
        "                             ,return_tensors=\"pt\"\n",
        "                             ,padding=True)\n",
        "            inputs = {key: value.to(device) for key, value in inputs.items()}\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = model(**inputs)\n",
        "                probs = outputs.logits_per_image.softmax(dim=1)\n",
        "\n",
        "            score = float(probs[0][0])\n",
        "            weighted_scores.append(score * weight)\n",
        "            total_weight += weight\n",
        "\n",
        "        consensus_score = sum(weighted_scores) / total_weight\n",
        "        results[label] = consensus_score\n",
        "\n",
        "    thresholds = {\"text\": 0.5, \"table\": 0.5}\n",
        "    predictions = {label: score > thresholds[label] for label, score in results.items()}\n",
        "\n",
        "    return {\n",
        "        \"model\": model_name,\n",
        "        \"predictions\": predictions,\n",
        "        \"scores\": results,\n",
        "        \"thresholds\": thresholds\n",
        "    }"
      ],
      "metadata": {
        "id": "8ws3xofVm4zM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_clip_grid_v3_3 = evaluate_binary_multilabel_model(\n",
        "    path_folder_test_baseline\n",
        "    ,ground_truth_pu_p01_pp01_multilabel\n",
        "    ,detect_multilabel_clip_grid_v3_3\n",
        "    ,processor_clip\n",
        "    ,model_clip\n",
        ")\n",
        "\n",
        "print(f\"\\n📊 Résultats CLIP:\")\n",
        "print(f\"F1 Text: {metrics_clip_grid_v3_3['f1_text']:.3f}\")\n",
        "print(f\"F1 Table: {metrics_clip_grid_v3_3['f1_table']:.3f}\")\n",
        "print(f\"F1 Macro: {metrics_clip_grid_v3_3['f1_macro']:.3f}\")\n",
        "print(f\"Hamming loss: {metrics_clip_grid_v3_3['hamming_loss']:.3f}\")\n",
        "print(f\"Jaccard: {metrics_clip_grid_v3_3['jaccard_macro']:.3f} & {metrics_clip_grid_v3_3['jaccard_micro']:.3f}\")\n",
        "print(f\"Jaccard samples :{metrics_clip_grid_v3_3['jaccard_samples']:.3f}\")\n",
        "print(f\"Jaccard per class: {metrics_clip_grid_v3_3['jaccard_per_class']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YEA5eyfonb6t",
        "outputId": "d42694d3-e755-45f0-bc1e-ed05b7a2278a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " p1: ✅ ✅ | Text:0.983 vs Table:0.418\n",
            " p2: ✅ ✅ | Text:0.934 vs Table:0.113\n",
            " p3: ✅ ✅ | Text:0.978 vs Table:0.511\n",
            " p4: ✅ ✅ | Text:0.964 vs Table:0.286\n",
            " p5: ✅ ❌ | Text:0.957 vs Table:0.612\n",
            " p6: ✅ ✅ | Text:0.969 vs Table:0.236\n",
            " p7: ✅ ✅ | Text:0.818 vs Table:0.494\n",
            " p8: ✅ ❌ | Text:0.889 vs Table:0.436\n",
            " p9: ✅ ✅ | Text:0.968 vs Table:0.229\n",
            " p10: ✅ ✅ | Text:0.962 vs Table:0.124\n",
            " p11: ✅ ✅ | Text:0.733 vs Table:0.221\n",
            " p12: ✅ ✅ | Text:0.948 vs Table:0.800\n",
            " p13: ✅ ✅ | Text:0.960 vs Table:0.511\n",
            " p14: ✅ ✅ | Text:0.976 vs Table:0.459\n",
            " p15: ✅ ❌ | Text:0.905 vs Table:0.220\n",
            " p16: ✅ ✅ | Text:0.941 vs Table:0.367\n",
            " p17: ✅ ✅ | Text:0.969 vs Table:0.193\n",
            " p18: ✅ ✅ | Text:0.968 vs Table:0.298\n",
            " p19: ✅ ✅ | Text:0.930 vs Table:0.216\n",
            " p20: ✅ ✅ | Text:0.975 vs Table:0.486\n",
            " p21: ✅ ✅ | Text:0.923 vs Table:0.380\n",
            " p22: ✅ ❌ | Text:0.873 vs Table:0.413\n",
            " p23: ✅ ✅ | Text:0.859 vs Table:0.671\n",
            " p24: ✅ ✅ | Text:0.884 vs Table:0.341\n",
            " p25: ✅ ✅ | Text:0.937 vs Table:0.273\n",
            " p26: ✅ ✅ | Text:0.964 vs Table:0.383\n",
            " p27: ✅ ✅ | Text:0.942 vs Table:0.406\n",
            " p28: ✅ ❌ | Text:0.842 vs Table:0.630\n",
            " p29: ✅ ✅ | Text:0.953 vs Table:0.248\n",
            " p30: ✅ ❌ | Text:0.917 vs Table:0.455\n",
            " p31: ✅ ❌ | Text:0.952 vs Table:0.284\n",
            " p32: ✅ ❌ | Text:0.987 vs Table:0.209\n",
            " p33: ✅ ✅ | Text:0.951 vs Table:0.355\n",
            " p34: ✅ ✅ | Text:0.934 vs Table:0.104\n",
            " p35: ✅ ❌ | Text:0.939 vs Table:0.385\n",
            " p36: ✅ ❌ | Text:0.919 vs Table:0.343\n",
            " p37: ✅ ❌ | Text:0.890 vs Table:0.481\n",
            " p38: ✅ ❌ | Text:0.982 vs Table:0.417\n",
            " p39: ✅ ✅ | Text:0.950 vs Table:0.191\n",
            " p40: ✅ ✅ | Text:0.966 vs Table:0.484\n",
            " p41: ✅ ✅ | Text:0.970 vs Table:0.760\n",
            " p42: ✅ ✅ | Text:0.914 vs Table:0.754\n",
            " p43: ✅ ✅ | Text:0.930 vs Table:0.875\n",
            " p44: ✅ ✅ | Text:0.958 vs Table:0.832\n",
            " p45: ✅ ✅ | Text:0.961 vs Table:0.789\n",
            " p46: ✅ ✅ | Text:0.886 vs Table:0.373\n",
            " p47: ✅ ✅ | Text:0.897 vs Table:0.662\n",
            " p48: ✅ ✅ | Text:0.865 vs Table:0.690\n",
            " p49: ✅ ✅ | Text:0.783 vs Table:0.843\n",
            "\n",
            "📊 Résultats CLIP:\n",
            "F1 Text: 1.000\n",
            "F1 Table: 0.667\n",
            "F1 Macro: 0.833\n",
            "Hamming loss: 0.122\n",
            "Jaccard: 0.750 & 0.836\n",
            "Jaccard samples :0.878\n",
            "Jaccard per class: [1.  0.5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_multilabel_clip_grid_v3_4(image_path, processor, model, model_name='CLIP v3.1 Weighted'):\n",
        "    \"\"\"CLIP consensus v3.1 avec poids optimisés basés sur l'analyse\"\"\"\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "    # Poids ajustés basés sur les performances observées\n",
        "    prompts_variants = {\n",
        "        \"text\": [\n",
        "            (\"document with printed text and readable content\", 1.0)\n",
        "        ],\n",
        "        \"table\": [\n",
        "            # Prompts v3 maintenus avec poids élevés (prouvés efficaces)\n",
        "            (\"administrative form with budget tables and financial data\", 1.2)     # ↑ Excellent sur Excel\n",
        "            ,(\"document with tabular information and structured data\", 1.9)        # ↓ Très bon généraliste\n",
        "            ,(\"document containing structured information in table format\", 1.1)   # ↑ Support solide\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    results = {}\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    for label, prompts_list in prompts_variants.items():\n",
        "        weighted_scores = []\n",
        "        total_weight = 0\n",
        "\n",
        "        for prompt, weight in prompts_list:\n",
        "            binary_prompts = [prompt, f\"document without {label}\"]\n",
        "\n",
        "            inputs = processor(text=binary_prompts\n",
        "                             ,images=image\n",
        "                             ,return_tensors=\"pt\"\n",
        "                             ,padding=True)\n",
        "            inputs = {key: value.to(device) for key, value in inputs.items()}\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = model(**inputs)\n",
        "                probs = outputs.logits_per_image.softmax(dim=1)\n",
        "\n",
        "            score = float(probs[0][0])\n",
        "            weighted_scores.append(score * weight)\n",
        "            total_weight += weight\n",
        "\n",
        "        consensus_score = sum(weighted_scores) / total_weight\n",
        "        results[label] = consensus_score\n",
        "\n",
        "    thresholds = {\"text\": 0.5, \"table\": 0.5}\n",
        "    predictions = {label: score > thresholds[label] for label, score in results.items()}\n",
        "\n",
        "    return {\n",
        "        \"model\": model_name,\n",
        "        \"predictions\": predictions,\n",
        "        \"scores\": results,\n",
        "        \"thresholds\": thresholds\n",
        "    }"
      ],
      "metadata": {
        "id": "kBNppvwyoViV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_clip_grid_v3_4 = evaluate_binary_multilabel_model(\n",
        "    path_folder_test_baseline\n",
        "    ,ground_truth_pu_p01_pp01_multilabel\n",
        "    ,detect_multilabel_clip_grid_v3_4\n",
        "    ,processor_clip\n",
        "    ,model_clip\n",
        ")\n",
        "\n",
        "print(f\"\\n📊 Résultats CLIP:\")\n",
        "print(f\"F1 Text: {metrics_clip_grid_v3_4['f1_text']:.3f}\")\n",
        "print(f\"F1 Table: {metrics_clip_grid_v3_4['f1_table']:.3f}\")\n",
        "print(f\"F1 Macro: {metrics_clip_grid_v3_4['f1_macro']:.3f}\")\n",
        "print(f\"Hamming loss: {metrics_clip_grid_v3_4['hamming_loss']:.3f}\")\n",
        "print(f\"Jaccard: {metrics_clip_grid_v3_4['jaccard_macro']:.3f} & {metrics_clip_grid_v3_4['jaccard_micro']:.3f}\")\n",
        "print(f\"Jaccard samples :{metrics_clip_grid_v3_4['jaccard_samples']:.3f}\")\n",
        "print(f\"Jaccard per class: {metrics_clip_grid_v3_4['jaccard_per_class']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CX1PO8KIptdo",
        "outputId": "d7915d7a-cbe4-4190-d73f-0ec95daaff25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " p1: ✅ ✅ | Text:0.983 vs Table:0.417\n",
            " p2: ✅ ✅ | Text:0.934 vs Table:0.113\n",
            " p3: ✅ ✅ | Text:0.978 vs Table:0.514\n",
            " p4: ✅ ✅ | Text:0.964 vs Table:0.283\n",
            " p5: ✅ ❌ | Text:0.957 vs Table:0.614\n",
            " p6: ✅ ✅ | Text:0.969 vs Table:0.237\n",
            " p7: ✅ ✅ | Text:0.818 vs Table:0.499\n",
            " p8: ✅ ❌ | Text:0.889 vs Table:0.430\n",
            " p9: ✅ ✅ | Text:0.968 vs Table:0.229\n",
            " p10: ✅ ✅ | Text:0.962 vs Table:0.125\n",
            " p11: ✅ ✅ | Text:0.733 vs Table:0.226\n",
            " p12: ✅ ✅ | Text:0.948 vs Table:0.799\n",
            " p13: ✅ ✅ | Text:0.960 vs Table:0.514\n",
            " p14: ✅ ✅ | Text:0.976 vs Table:0.458\n",
            " p15: ✅ ✅ | Text:0.905 vs Table:0.222\n",
            " p16: ✅ ✅ | Text:0.941 vs Table:0.373\n",
            " p17: ✅ ✅ | Text:0.969 vs Table:0.193\n",
            " p18: ✅ ✅ | Text:0.968 vs Table:0.299\n",
            " p19: ✅ ✅ | Text:0.930 vs Table:0.213\n",
            " p20: ✅ ✅ | Text:0.975 vs Table:0.484\n",
            " p21: ✅ ✅ | Text:0.923 vs Table:0.377\n",
            " p22: ✅ ❌ | Text:0.873 vs Table:0.407\n",
            " p23: ✅ ✅ | Text:0.859 vs Table:0.664\n",
            " p24: ✅ ✅ | Text:0.884 vs Table:0.339\n",
            " p25: ✅ ✅ | Text:0.937 vs Table:0.271\n",
            " p26: ✅ ✅ | Text:0.964 vs Table:0.381\n",
            " p27: ✅ ✅ | Text:0.942 vs Table:0.406\n",
            " p28: ✅ ❌ | Text:0.842 vs Table:0.632\n",
            " p29: ✅ ✅ | Text:0.953 vs Table:0.253\n",
            " p30: ✅ ❌ | Text:0.917 vs Table:0.450\n",
            " p31: ✅ ❌ | Text:0.952 vs Table:0.279\n",
            " p32: ✅ ❌ | Text:0.987 vs Table:0.211\n",
            " p33: ✅ ✅ | Text:0.951 vs Table:0.353\n",
            " p34: ✅ ✅ | Text:0.934 vs Table:0.104\n",
            " p35: ✅ ❌ | Text:0.939 vs Table:0.381\n",
            " p36: ✅ ❌ | Text:0.919 vs Table:0.339\n",
            " p37: ✅ ❌ | Text:0.890 vs Table:0.477\n",
            " p38: ✅ ❌ | Text:0.982 vs Table:0.416\n",
            " p39: ✅ ✅ | Text:0.950 vs Table:0.192\n",
            " p40: ✅ ✅ | Text:0.966 vs Table:0.482\n",
            " p41: ✅ ✅ | Text:0.970 vs Table:0.761\n",
            " p42: ✅ ✅ | Text:0.914 vs Table:0.755\n",
            " p43: ✅ ✅ | Text:0.930 vs Table:0.875\n",
            " p44: ✅ ✅ | Text:0.958 vs Table:0.832\n",
            " p45: ✅ ✅ | Text:0.961 vs Table:0.790\n",
            " p46: ✅ ✅ | Text:0.886 vs Table:0.378\n",
            " p47: ✅ ✅ | Text:0.897 vs Table:0.665\n",
            " p48: ✅ ✅ | Text:0.865 vs Table:0.694\n",
            " p49: ✅ ✅ | Text:0.783 vs Table:0.846\n",
            "\n",
            "📊 Résultats CLIP:\n",
            "F1 Text: 1.000\n",
            "F1 Table: 0.686\n",
            "F1 Macro: 0.843\n",
            "Hamming loss: 0.112\n",
            "Jaccard: 0.761 & 0.847\n",
            "Jaccard samples :0.888\n",
            "Jaccard per class: [1.         0.52173913]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_multilabel_clip_grid_v3_5(image_path, processor, model, model_name='CLIP v3.5 Weighted'):\n",
        "    \"\"\"CLIP consensus v3.1 avec poids optimisés basés sur l'analyse\"\"\"\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "    # Poids ajustés basés sur les performances observées\n",
        "    prompts_variants = {\n",
        "        \"text\": [\n",
        "            (\"document with printed text and readable content\", 1.0)\n",
        "        ],\n",
        "        \"table\": [\n",
        "            # Prompts v3 maintenus avec poids élevés (prouvés efficaces)\n",
        "            (\"administrative form with budget tables and financial data\", 1.2)     # ↑ Excellent sur Excel\n",
        "            ,(\"document with tabular information and structured data\", 1.9)        # ↓ Très bon généraliste\n",
        "            ,(\"document containing structured information in table format\", 1.1)   # ↑ Support solide\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    results = {}\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    for label, prompts_list in prompts_variants.items():\n",
        "        weighted_scores = []\n",
        "        total_weight = 0\n",
        "\n",
        "        for prompt, weight in prompts_list:\n",
        "            binary_prompts = [prompt, f\"document without {label}\"]\n",
        "\n",
        "            inputs = processor(text=binary_prompts\n",
        "                             ,images=image\n",
        "                             ,return_tensors=\"pt\"\n",
        "                             ,padding=True)\n",
        "            inputs = {key: value.to(device) for key, value in inputs.items()}\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = model(**inputs)\n",
        "                probs = outputs.logits_per_image.softmax(dim=1)\n",
        "\n",
        "            score = float(probs[0][0])\n",
        "            weighted_scores.append(score * weight)\n",
        "            total_weight += weight\n",
        "\n",
        "        consensus_score = sum(weighted_scores) / total_weight\n",
        "        results[label] = consensus_score\n",
        "\n",
        "    thresholds = {\"text\": 0.5, \"table\": 0.5}\n",
        "    predictions = {label: score > thresholds[label] for label, score in results.items()}\n",
        "\n",
        "    return {\n",
        "        \"model\": model_name,\n",
        "        \"predictions\": predictions,\n",
        "        \"scores\": results,\n",
        "        \"thresholds\": thresholds\n",
        "    }"
      ],
      "metadata": {
        "id": "bAaRqLKXqLRX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_clip_grid_v3_5 = evaluate_binary_multilabel_model(\n",
        "    path_folder_test_baseline\n",
        "    ,ground_truth_pu_p01_pp01_multilabel\n",
        "    ,detect_multilabel_clip_grid_v3_5\n",
        "    ,processor_clip\n",
        "    ,model_clip\n",
        ")\n",
        "\n",
        "print(f\"\\n📊 Résultats CLIP:\")\n",
        "print(f\"F1 Text: {metrics_clip_grid_v3_5['f1_text']:.3f}\")\n",
        "print(f\"F1 Table: {metrics_clip_grid_v3_5['f1_table']:.3f}\")\n",
        "print(f\"F1 Macro: {metrics_clip_grid_v3_5['f1_macro']:.3f}\")\n",
        "print(f\"Hamming loss: {metrics_clip_grid_v3_5['hamming_loss']:.3f}\")\n",
        "print(f\"Jaccard: {metrics_clip_grid_v3_5['jaccard_macro']:.3f} & {metrics_clip_grid_v3_5['jaccard_micro']:.3f}\")\n",
        "print(f\"Jaccard samples :{metrics_clip_grid_v3_5['jaccard_samples']:.3f}\")\n",
        "print(f\"Jaccard per class: {metrics_clip_grid_v3_5['jaccard_per_class']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5FDQp3YpqUl9",
        "outputId": "d5baa62e-da5a-4c15-cce2-9d3bd56089e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " p1: ✅ ✅ | Text:0.983 vs Table:0.417\n",
            " p2: ✅ ✅ | Text:0.934 vs Table:0.113\n",
            " p3: ✅ ✅ | Text:0.978 vs Table:0.514\n",
            " p4: ✅ ✅ | Text:0.964 vs Table:0.283\n",
            " p5: ✅ ❌ | Text:0.957 vs Table:0.614\n",
            " p6: ✅ ✅ | Text:0.969 vs Table:0.237\n",
            " p7: ✅ ✅ | Text:0.818 vs Table:0.499\n",
            " p8: ✅ ❌ | Text:0.889 vs Table:0.430\n",
            " p9: ✅ ✅ | Text:0.968 vs Table:0.229\n",
            " p10: ✅ ✅ | Text:0.962 vs Table:0.125\n",
            " p11: ✅ ✅ | Text:0.733 vs Table:0.226\n",
            " p12: ✅ ✅ | Text:0.948 vs Table:0.799\n",
            " p13: ✅ ✅ | Text:0.960 vs Table:0.514\n",
            " p14: ✅ ✅ | Text:0.976 vs Table:0.458\n",
            " p15: ✅ ❌ | Text:0.905 vs Table:0.222\n",
            " p16: ✅ ✅ | Text:0.941 vs Table:0.373\n",
            " p17: ✅ ✅ | Text:0.969 vs Table:0.193\n",
            " p18: ✅ ✅ | Text:0.968 vs Table:0.299\n",
            " p19: ✅ ✅ | Text:0.930 vs Table:0.213\n",
            " p20: ✅ ✅ | Text:0.975 vs Table:0.484\n",
            " p21: ✅ ✅ | Text:0.923 vs Table:0.377\n",
            " p22: ✅ ❌ | Text:0.873 vs Table:0.407\n",
            " p23: ✅ ✅ | Text:0.859 vs Table:0.664\n",
            " p24: ✅ ✅ | Text:0.884 vs Table:0.339\n",
            " p25: ✅ ✅ | Text:0.937 vs Table:0.271\n",
            " p26: ✅ ✅ | Text:0.964 vs Table:0.381\n",
            " p27: ✅ ✅ | Text:0.942 vs Table:0.406\n",
            " p28: ✅ ❌ | Text:0.842 vs Table:0.632\n",
            " p29: ✅ ✅ | Text:0.953 vs Table:0.253\n",
            " p30: ✅ ❌ | Text:0.917 vs Table:0.450\n",
            " p31: ✅ ❌ | Text:0.952 vs Table:0.279\n",
            " p32: ✅ ❌ | Text:0.987 vs Table:0.211\n",
            " p33: ✅ ✅ | Text:0.951 vs Table:0.353\n",
            " p34: ✅ ✅ | Text:0.934 vs Table:0.104\n",
            " p35: ✅ ❌ | Text:0.939 vs Table:0.381\n",
            " p36: ✅ ❌ | Text:0.919 vs Table:0.339\n",
            " p37: ✅ ❌ | Text:0.890 vs Table:0.477\n",
            " p38: ✅ ❌ | Text:0.982 vs Table:0.416\n",
            " p39: ✅ ✅ | Text:0.950 vs Table:0.192\n",
            " p40: ✅ ✅ | Text:0.966 vs Table:0.482\n",
            " p41: ✅ ✅ | Text:0.970 vs Table:0.761\n",
            " p42: ✅ ✅ | Text:0.914 vs Table:0.755\n",
            " p43: ✅ ✅ | Text:0.930 vs Table:0.875\n",
            " p44: ✅ ✅ | Text:0.958 vs Table:0.832\n",
            " p45: ✅ ✅ | Text:0.961 vs Table:0.790\n",
            " p46: ✅ ✅ | Text:0.886 vs Table:0.378\n",
            " p47: ✅ ✅ | Text:0.897 vs Table:0.665\n",
            " p48: ✅ ✅ | Text:0.865 vs Table:0.694\n",
            " p49: ✅ ✅ | Text:0.783 vs Table:0.846\n",
            "\n",
            "📊 Résultats CLIP:\n",
            "F1 Text: 1.000\n",
            "F1 Table: 0.667\n",
            "F1 Macro: 0.833\n",
            "Hamming loss: 0.122\n",
            "Jaccard: 0.750 & 0.836\n",
            "Jaccard samples :0.878\n",
            "Jaccard per class: [1.  0.5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Amélioration des prompts pour détection tableau Excel-like"
      ],
      "metadata": {
        "id": "Q0UqmaMIQM6z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test rapide prompt Excel sur les 5 pages restantes\n",
        "excel_pages = [\"p3\",\"p8\",\"p15\", \"p32\", \"p38\"]\n",
        "\n",
        "excel_prompts = [\n",
        "    \"administrative form with budget tables and financial data\",\n",
        "    \"document with tabular information and structured data layout\",\n",
        "    \"spreadsheet-like table with rows and columns of textual and numerical data, commonly found in financial, budgetary, or informational reports, featuring structured headers and organized content for clear data presentation\",\n",
        "]\n",
        "\n",
        "print(\"🧪 TEST PROMPTS EXCEL - Pages problématiques\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "def analyze_prompt_effectiveness_custom(image_path, processor, model, custom_prompts):\n",
        "    \"\"\"Version custom avec prompts spécifiques\"\"\"\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "    results = []\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    for i, prompt in enumerate(custom_prompts):\n",
        "        binary_prompts = [prompt, \"document without table\"]\n",
        "\n",
        "        inputs = processor(text=binary_prompts, images=image, return_tensors=\"pt\", padding=True)\n",
        "        inputs = {key: value.to(device) for key, value in inputs.items()}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "            probs = outputs.logits_per_image.softmax(dim=1)\n",
        "\n",
        "        score = float(probs[0][0])\n",
        "        results.append((i, prompt, score))\n",
        "\n",
        "    # Tri par efficacité\n",
        "    results.sort(key=lambda x: x[2], reverse=True)\n",
        "    return results\n",
        "\n",
        "for page in excel_pages:\n",
        "    print(f\"\\n📊 {page} - Analysis Excel prompts:\")\n",
        "    analysis = analyze_prompt_effectiveness_custom(path_pu_p01_pp01[page], processor_clip, model_clip, excel_prompts)\n",
        "    for rank, (idx, prompt, score) in enumerate(analysis):\n",
        "        print(f\"  {rank+1}. [{idx}] {score:.3f} - {prompt[:50]}...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y5KSSe_6xML0",
        "outputId": "1c974223-4044-45d1-f5ac-9c0613cbe187"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧪 TEST PROMPTS EXCEL - Pages problématiques\n",
            "============================================================\n",
            "\n",
            "📊 p3 - Analysis Excel prompts:\n",
            "  1. [1] 0.727 - document with tabular information and structured d...\n",
            "  2. [0] 0.461 - administrative form with budget tables and financi...\n",
            "  3. [2] 0.315 - spreadsheet-like table with rows and columns of te...\n",
            "\n",
            "📊 p8 - Analysis Excel prompts:\n",
            "  1. [0] 0.679 - administrative form with budget tables and financi...\n",
            "  2. [1] 0.272 - document with tabular information and structured d...\n",
            "  3. [2] 0.233 - spreadsheet-like table with rows and columns of te...\n",
            "\n",
            "📊 p15 - Analysis Excel prompts:\n",
            "  1. [0] 0.285 - administrative form with budget tables and financi...\n",
            "  2. [1] 0.173 - document with tabular information and structured d...\n",
            "  3. [2] 0.096 - spreadsheet-like table with rows and columns of te...\n",
            "\n",
            "📊 p32 - Analysis Excel prompts:\n",
            "  1. [1] 0.242 - document with tabular information and structured d...\n",
            "  2. [0] 0.084 - administrative form with budget tables and financi...\n",
            "  3. [2] 0.050 - spreadsheet-like table with rows and columns of te...\n",
            "\n",
            "📊 p38 - Analysis Excel prompts:\n",
            "  1. [1] 0.504 - document with tabular information and structured d...\n",
            "  2. [2] 0.430 - spreadsheet-like table with rows and columns of te...\n",
            "  3. [0] 0.398 - administrative form with budget tables and financi...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Réflexion pour amélioration détection Excel\n",
        "\n",
        "On garde ces 2 prompts\n",
        "\n",
        "-\"administrative form with budget tables and financial data\"\n",
        "\n",
        "-\"document with tabular information and structured data\"\n",
        "\n",
        "Test excel_v4_1\n",
        "- avec uniquement ces 2 prompts\n",
        "- 2 prompts + 1 autre\n",
        "- 2 nouveaux prompts\n",
        "- 2 prompts validés + 1 prompt infographie ? ou on laisse tomber (attente de validation de Aghiles)\n",
        "\n",
        "Quid du poids ?"
      ],
      "metadata": {
        "id": "vvW86I78QsEt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_multilabel_clip_excel_v4_1(image_path, processor, model, model_name='CLIP v4 Excel'):\n",
        "    \"\"\"CLIP consensus v4.1 avec multiprompt pour détection tableau Excel-like\"\"\"\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "    # Poids ajustés basés sur les performances observées\n",
        "    prompts_variants = {\n",
        "        \"text\": [\n",
        "            (\"document with printed text, paragraphs, bullet point lists, and readable content, without grid\", 1.0)\n",
        "        ],\n",
        "        \"table\": [\n",
        "            # Prompts v3 maintenus avec poids élevés (prouvés efficaces)\n",
        "            (\"administrative form with budget tables and financial data\", 1.1)\n",
        "            ,(\"spreadsheet-like table with rows and columns of textual and numerical data, commonly found in financial, budgetary, or informational reports, featuring structured headers and organized content for clear data presentation\", 1.0)\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    results = {}\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    for label, prompts_list in prompts_variants.items():\n",
        "        weighted_scores = []\n",
        "        total_weight = 0\n",
        "\n",
        "        for prompt, weight in prompts_list:\n",
        "            binary_prompts = [prompt, f\"document without {label}\"]\n",
        "\n",
        "            inputs = processor(text=binary_prompts\n",
        "                             ,images=image\n",
        "                             ,return_tensors=\"pt\"\n",
        "                             ,padding=True)\n",
        "            inputs = {key: value.to(device) for key, value in inputs.items()}\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = model(**inputs)\n",
        "                probs = outputs.logits_per_image.softmax(dim=1)\n",
        "\n",
        "            score = float(probs[0][0])\n",
        "            weighted_scores.append(score * weight)\n",
        "            total_weight += weight\n",
        "\n",
        "        # print(f\"Weighted scores: {weighted_scores}\")\n",
        "        # print(f\"Total weight: {total_weight}\")\n",
        "\n",
        "        consensus_score = sum(weighted_scores) / total_weight\n",
        "        results[label] = consensus_score\n",
        "\n",
        "        # print(f\"Consensus: {consensus_score:.3f}\")\n",
        "\n",
        "\n",
        "\n",
        "    thresholds = {\"text\": 0.5, \"table\": 0.5}\n",
        "    predictions = {label: score > thresholds[label] for label, score in results.items()}\n",
        "\n",
        "    return {\n",
        "        \"model\": model_name,\n",
        "        \"predictions\": predictions,\n",
        "        \"scores\": results,\n",
        "        \"thresholds\": thresholds\n",
        "    }"
      ],
      "metadata": {
        "id": "SjgwCRMMQqds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_clip_excel_v4_1 = evaluate_binary_multilabel_model(\n",
        "    path_folder_test_baseline\n",
        "    ,ground_truth_pu_p01_pp01_multilabel\n",
        "    ,detect_multilabel_clip_excel_v4_1\n",
        "    ,processor_clip\n",
        "    ,model_clip\n",
        ")\n",
        "\n",
        "print(f\"\\n📊 Résultats CLIP:\")\n",
        "print(f\"F1 Text: {metrics_clip_excel_v4_1['f1_text']:.3f}\")\n",
        "print(f\"F1 Table: {metrics_clip_excel_v4_1['f1_table']:.3f}\")\n",
        "print(f\"F1 Macro: {metrics_clip_excel_v4_1['f1_macro']:.3f}\")\n",
        "print(f\"Hamming loss: {metrics_clip_excel_v4_1['hamming_loss']:.3f}\")\n",
        "print(f\"Jaccard: {metrics_clip_excel_v4_1['jaccard_macro']:.3f} & {metrics_clip_excel_v4_1['jaccard_micro']:.3f}\")\n",
        "print(f\"Jaccard samples :{metrics_clip_excel_v4_1['jaccard_samples']:.3f}\")\n",
        "print(f\"Jaccard per class: {metrics_clip_excel_v4_1['jaccard_per_class']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3R9cHEh_HfC",
        "outputId": "5fface40-41c5-4559-ca57-4662cb6fe78f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " p1: ✅ ✅ | Text:0.878 vs Table:0.493\n",
            " p2: ✅ ✅ | Text:0.813 vs Table:0.072\n",
            " p3: ✅ ❌ | Text:0.982 vs Table:0.391\n",
            " p4: ✅ ✅ | Text:0.900 vs Table:0.255\n",
            " p5: ✅ ✅ | Text:0.945 vs Table:0.421\n",
            " p6: ✅ ✅ | Text:0.947 vs Table:0.089\n",
            " p7: ✅ ✅ | Text:0.896 vs Table:0.327\n",
            " p8: ✅ ❌ | Text:0.791 vs Table:0.467\n",
            " p9: ✅ ✅ | Text:0.940 vs Table:0.139\n",
            " p10: ✅ ✅ | Text:0.873 vs Table:0.037\n",
            " p11: ✅ ✅ | Text:0.507 vs Table:0.011\n",
            " p12: ✅ ✅ | Text:0.937 vs Table:0.818\n",
            " p13: ✅ ✅ | Text:0.968 vs Table:0.515\n",
            " p14: ✅ ✅ | Text:0.981 vs Table:0.300\n",
            " p15: ✅ ✅ | Text:0.963 vs Table:0.195\n",
            " p16: ✅ ✅ | Text:0.969 vs Table:0.140\n",
            " p17: ✅ ✅ | Text:0.876 vs Table:0.080\n",
            " p18: ✅ ✅ | Text:0.833 vs Table:0.044\n",
            " p19: ✅ ✅ | Text:0.829 vs Table:0.104\n",
            " p20: ✅ ✅ | Text:0.970 vs Table:0.407\n",
            " p21: ✅ ✅ | Text:0.873 vs Table:0.291\n",
            " p22: ✅ ✅ | Text:0.751 vs Table:0.512\n",
            " p23: ✅ ✅ | Text:0.832 vs Table:0.984\n",
            " p24: ✅ ✅ | Text:0.884 vs Table:0.431\n",
            " p25: ✅ ✅ | Text:0.957 vs Table:0.300\n",
            " p26: ✅ ✅ | Text:0.929 vs Table:0.384\n",
            " p27: ✅ ✅ | Text:0.951 vs Table:0.356\n",
            " p28: ✅ ✅ | Text:0.878 vs Table:0.366\n",
            " p29: ✅ ✅ | Text:0.945 vs Table:0.042\n",
            " p30: ✅ ✅ | Text:0.836 vs Table:0.517\n",
            " p31: ✅ ✅ | Text:0.943 vs Table:0.511\n",
            " p32: ✅ ❌ | Text:0.967 vs Table:0.068\n",
            " p33: ✅ ✅ | Text:0.919 vs Table:0.229\n",
            " p34: ✅ ✅ | Text:0.784 vs Table:0.057\n",
            " p35: ✅ ✅ | Text:0.906 vs Table:0.539\n",
            " p36: ✅ ✅ | Text:0.919 vs Table:0.565\n",
            " p37: ✅ ✅ | Text:0.881 vs Table:0.675\n",
            " p38: ✅ ❌ | Text:0.977 vs Table:0.413\n",
            " p39: ✅ ✅ | Text:0.888 vs Table:0.080\n",
            " p40: ✅ ✅ | Text:0.920 vs Table:0.197\n",
            " p41: ✅ ✅ | Text:0.981 vs Table:0.787\n",
            " p42: ✅ ✅ | Text:0.966 vs Table:0.852\n",
            " p43: ✅ ✅ | Text:0.975 vs Table:0.959\n",
            " p44: ✅ ✅ | Text:0.985 vs Table:0.901\n",
            " p45: ✅ ✅ | Text:0.992 vs Table:0.927\n",
            " p46: ✅ ✅ | Text:0.961 vs Table:0.248\n",
            " p47: ✅ ✅ | Text:0.989 vs Table:0.858\n",
            " p48: ✅ ✅ | Text:0.992 vs Table:0.949\n",
            " p49: ✅ ✅ | Text:0.967 vs Table:0.858\n",
            "\n",
            "📊 Résultats CLIP:\n",
            "F1 Text: 1.000\n",
            "F1 Table: 0.895\n",
            "F1 Macro: 0.947\n",
            "Hamming loss: 0.041\n",
            "Jaccard: 0.905 & 0.943\n",
            "Jaccard samples :0.959\n",
            "Jaccard per class: [1.         0.80952381]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Thinking note\n",
        "\n",
        "si on reprend les résultats \"TEST PROMPTS EXCEL - Pages problématiques\" et les poids cela devrait donner\n",
        "\n",
        "- p3: (0.721×1.2 + 0.582×1.1) / 2.3 = 0.657\n",
        "→ Devrait passer le seuil 0.5 !\n",
        "\n",
        "- p8: (0.860×1.2 + 0.469×1.1) / 2.3 = 0.672\n",
        "→ Devrait passer aussi !\n",
        "\n",
        "- p38: (0.576×1.1 + 0.543×1.2) / 2.3 = 0.559\n",
        "→ ici aussi\n",
        "\n",
        "Il n'y a plus de faux positif c'est une excellente nouvelle.\n",
        "\n",
        "- p15: c'est l'infographie (modification du ground truth Table:False)\n",
        "- p32 : c'est une fin de tableau\n",
        "\n",
        "\n",
        "Erreur trouvée !\n",
        "dans l'analyse, il y a une coquille dans le consensus, j'ai écrit:\n",
        "- binary_prompts = [prompt, \"document without tables\"] → mauvais\n",
        "- binary_prompts = [prompt, \"document without table\"] → bon\n",
        "\n",
        "les résultats sont cohérents."
      ],
      "metadata": {
        "id": "c6B6K2MuQP0B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result_p3 = detect_multilabel_clip_excel_v4_1(\"/content/drive/MyDrive/Document AI - GroupeSOS/Outputs/Save_img_from_pdf/PU_P01_PP01_folder/PU_P01_PP01_page_3.jpg\", processor_clip, model_clip)\n",
        "print(f\"Scores p3: {result_p3['scores']}\")\n",
        "print(f\"Table score: {result_p3['scores']['table']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Up-QCzdAOrvF",
        "outputId": "2d503ce2-d1dc-4095-dca1-9c91c82076a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scores p3: {'text': 0.9820206761360168, 'table': 0.391020461269047}\n",
            "Table score: 0.391020461269047\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ajout de la classe \"Schema\""
      ],
      "metadata": {
        "id": "T6RHbP-bcym3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ajout d'un 2ème prompt ?\n",
        "\n",
        "schema_pages = [\"p15\",\"p47\",\"p48\",\"p49\"]\n",
        "\n",
        "for page in schema_pages:\n",
        "    print(f\"\\n📊 {page} - Analysis p15 prompts:\")\n",
        "    analysis = analyze_prompt_effectiveness_p15(path_pu_p01_pp01[page], processor_clip, model_clip)\n",
        "    for rank, (score, prompt) in enumerate(analysis):\n",
        "        print(f\"  {rank+1}. {score:.3f} - {prompt[:50]}...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9EUNWBZJ4iUI",
        "outputId": "61640803-3dce-42d2-e1cd-4baf7db23c64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📊 p15 - Analysis p15 prompts:\n",
            "🔬 ANALYSE SPÉCIALISÉE p15\n",
            "==================================================\n",
            "   0.998 | business process flowchart with colored sections and directional arrows, not planning schedule\n",
            "   0.300 | flowchart with interconnected boxes and directional flow arrows\n",
            "   0.208 | diagram showing process relationships with connecting arrows\n",
            "   0.602 | business logic diagram with linked components and flow direction\n",
            "   0.921 | process flowchart with connections, not scheduling document\n",
            "   0.879 | conceptual diagram with linked elements, not timeline or calendar\n",
            "   0.968 | workflow diagram with process flow, not planning grid\n",
            "   0.981 | conceptual framework diagram with connected logical elements\n",
            "   0.935 | process model showing relationships between different components\n",
            "\n",
            "🏆 RANKING:\n",
            "    1. 0.998 | business process flowchart with colored sections and directional arrows, not planning schedule\n",
            "    2. 0.981 | conceptual framework diagram with connected logical elements\n",
            "    3. 0.968 | workflow diagram with process flow, not planning grid\n",
            "    4. 0.935 | process model showing relationships between different components\n",
            "    5. 0.921 | process flowchart with connections, not scheduling document\n",
            "    6. 0.879 | conceptual diagram with linked elements, not timeline or calendar\n",
            "    7. 0.602 | business logic diagram with linked components and flow direction\n",
            "    8. 0.300 | flowchart with interconnected boxes and directional flow arrows\n",
            "    9. 0.208 | diagram showing process relationships with connecting arrows\n",
            "  1. 0.998 - business process flowchart with colored sections a...\n",
            "  2. 0.981 - conceptual framework diagram with connected logica...\n",
            "  3. 0.968 - workflow diagram with process flow, not planning g...\n",
            "  4. 0.935 - process model showing relationships between differ...\n",
            "  5. 0.921 - process flowchart with connections, not scheduling...\n",
            "  6. 0.879 - conceptual diagram with linked elements, not timel...\n",
            "  7. 0.602 - business logic diagram with linked components and ...\n",
            "  8. 0.300 - flowchart with interconnected boxes and directiona...\n",
            "  9. 0.208 - diagram showing process relationships with connect...\n",
            "\n",
            "📊 p47 - Analysis p15 prompts:\n",
            "🔬 ANALYSE SPÉCIALISÉE p15\n",
            "==================================================\n",
            "   0.998 | business process flowchart with colored sections and directional arrows, not planning schedule\n",
            "   0.529 | flowchart with interconnected boxes and directional flow arrows\n",
            "   0.108 | diagram showing process relationships with connecting arrows\n",
            "   0.258 | business logic diagram with linked components and flow direction\n",
            "   0.965 | process flowchart with connections, not scheduling document\n",
            "   0.969 | conceptual diagram with linked elements, not timeline or calendar\n",
            "   0.970 | workflow diagram with process flow, not planning grid\n",
            "   0.723 | conceptual framework diagram with connected logical elements\n",
            "   0.572 | process model showing relationships between different components\n",
            "\n",
            "🏆 RANKING:\n",
            "    1. 0.998 | business process flowchart with colored sections and directional arrows, not planning schedule\n",
            "    2. 0.970 | workflow diagram with process flow, not planning grid\n",
            "    3. 0.969 | conceptual diagram with linked elements, not timeline or calendar\n",
            "    4. 0.965 | process flowchart with connections, not scheduling document\n",
            "    5. 0.723 | conceptual framework diagram with connected logical elements\n",
            "    6. 0.572 | process model showing relationships between different components\n",
            "    7. 0.529 | flowchart with interconnected boxes and directional flow arrows\n",
            "    8. 0.258 | business logic diagram with linked components and flow direction\n",
            "    9. 0.108 | diagram showing process relationships with connecting arrows\n",
            "  1. 0.998 - business process flowchart with colored sections a...\n",
            "  2. 0.970 - workflow diagram with process flow, not planning g...\n",
            "  3. 0.969 - conceptual diagram with linked elements, not timel...\n",
            "  4. 0.965 - process flowchart with connections, not scheduling...\n",
            "  5. 0.723 - conceptual framework diagram with connected logica...\n",
            "  6. 0.572 - process model showing relationships between differ...\n",
            "  7. 0.529 - flowchart with interconnected boxes and directiona...\n",
            "  8. 0.258 - business logic diagram with linked components and ...\n",
            "  9. 0.108 - diagram showing process relationships with connect...\n",
            "\n",
            "📊 p48 - Analysis p15 prompts:\n",
            "🔬 ANALYSE SPÉCIALISÉE p15\n",
            "==================================================\n",
            "   0.956 | business process flowchart with colored sections and directional arrows, not planning schedule\n",
            "   0.176 | flowchart with interconnected boxes and directional flow arrows\n",
            "   0.105 | diagram showing process relationships with connecting arrows\n",
            "   0.490 | business logic diagram with linked components and flow direction\n",
            "   0.870 | process flowchart with connections, not scheduling document\n",
            "   0.897 | conceptual diagram with linked elements, not timeline or calendar\n",
            "   0.920 | workflow diagram with process flow, not planning grid\n",
            "   0.933 | conceptual framework diagram with connected logical elements\n",
            "   0.749 | process model showing relationships between different components\n",
            "\n",
            "🏆 RANKING:\n",
            "    1. 0.956 | business process flowchart with colored sections and directional arrows, not planning schedule\n",
            "    2. 0.933 | conceptual framework diagram with connected logical elements\n",
            "    3. 0.920 | workflow diagram with process flow, not planning grid\n",
            "    4. 0.897 | conceptual diagram with linked elements, not timeline or calendar\n",
            "    5. 0.870 | process flowchart with connections, not scheduling document\n",
            "    6. 0.749 | process model showing relationships between different components\n",
            "    7. 0.490 | business logic diagram with linked components and flow direction\n",
            "    8. 0.176 | flowchart with interconnected boxes and directional flow arrows\n",
            "    9. 0.105 | diagram showing process relationships with connecting arrows\n",
            "  1. 0.956 - business process flowchart with colored sections a...\n",
            "  2. 0.933 - conceptual framework diagram with connected logica...\n",
            "  3. 0.920 - workflow diagram with process flow, not planning g...\n",
            "  4. 0.897 - conceptual diagram with linked elements, not timel...\n",
            "  5. 0.870 - process flowchart with connections, not scheduling...\n",
            "  6. 0.749 - process model showing relationships between differ...\n",
            "  7. 0.490 - business logic diagram with linked components and ...\n",
            "  8. 0.176 - flowchart with interconnected boxes and directiona...\n",
            "  9. 0.105 - diagram showing process relationships with connect...\n",
            "\n",
            "📊 p49 - Analysis p15 prompts:\n",
            "🔬 ANALYSE SPÉCIALISÉE p15\n",
            "==================================================\n",
            "   0.930 | business process flowchart with colored sections and directional arrows, not planning schedule\n",
            "   0.118 | flowchart with interconnected boxes and directional flow arrows\n",
            "   0.140 | diagram showing process relationships with connecting arrows\n",
            "   0.813 | business logic diagram with linked components and flow direction\n",
            "   0.913 | process flowchart with connections, not scheduling document\n",
            "   0.893 | conceptual diagram with linked elements, not timeline or calendar\n",
            "   0.919 | workflow diagram with process flow, not planning grid\n",
            "   0.626 | conceptual framework diagram with connected logical elements\n",
            "   0.794 | process model showing relationships between different components\n",
            "\n",
            "🏆 RANKING:\n",
            "    1. 0.930 | business process flowchart with colored sections and directional arrows, not planning schedule\n",
            "    2. 0.919 | workflow diagram with process flow, not planning grid\n",
            "    3. 0.913 | process flowchart with connections, not scheduling document\n",
            "    4. 0.893 | conceptual diagram with linked elements, not timeline or calendar\n",
            "    5. 0.813 | business logic diagram with linked components and flow direction\n",
            "    6. 0.794 | process model showing relationships between different components\n",
            "    7. 0.626 | conceptual framework diagram with connected logical elements\n",
            "    8. 0.140 | diagram showing process relationships with connecting arrows\n",
            "    9. 0.118 | flowchart with interconnected boxes and directional flow arrows\n",
            "  1. 0.930 - business process flowchart with colored sections a...\n",
            "  2. 0.919 - workflow diagram with process flow, not planning g...\n",
            "  3. 0.913 - process flowchart with connections, not scheduling...\n",
            "  4. 0.893 - conceptual diagram with linked elements, not timel...\n",
            "  5. 0.813 - business logic diagram with linked components and ...\n",
            "  6. 0.794 - process model showing relationships between differ...\n",
            "  7. 0.626 - conceptual framework diagram with connected logica...\n",
            "  8. 0.140 - diagram showing process relationships with connect...\n",
            "  9. 0.118 - flowchart with interconnected boxes and directiona...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_multilabel_clip_schema_v5(image_path, processor, model, model_name='CLIP v5_schema'):\n",
        "    \"\"\"CLIP consensus v5 avec multiprompt pour détection Schema / infographie\"\"\"\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "    # Poids ajustés basés sur les performances observées\n",
        "    prompts_variants = {\n",
        "        \"text\": [\n",
        "            (\"document with printed text, paragraphs, bullet point lists, and readable content, without grid\", 1.0)\n",
        "        ],\n",
        "        \"table\": [\n",
        "            # Prompts v3 maintenus avec poids élevés (prouvés efficaces)\n",
        "            (\"administrative form with budget tables and financial data\", 1.1)\n",
        "            ,(\"spreadsheet-like table with rows and columns of textual and numerical data, commonly found in financial, budgetary, or informational reports, featuring structured headers and organized content for clear data presentation\", 1.0)\n",
        "        ],\n",
        "        \"schema\": [\n",
        "            (\"diagram with colored boxes connected by arrows\",1.1)\n",
        "            ,(\"infographic map with visual elements and legend\",1.3)\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    results = {}\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    for label, prompts_list in prompts_variants.items():\n",
        "        weighted_scores = []\n",
        "        total_weight = 0\n",
        "\n",
        "        for prompt, weight in prompts_list:\n",
        "            binary_prompts = [prompt, f\"document without {label}\"]\n",
        "\n",
        "            inputs = processor(text=binary_prompts\n",
        "                             ,images=image\n",
        "                             ,return_tensors=\"pt\"\n",
        "                             ,padding=True)\n",
        "            inputs = {key: value.to(device) for key, value in inputs.items()}\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = model(**inputs)\n",
        "                probs = outputs.logits_per_image.softmax(dim=1)\n",
        "\n",
        "            score = float(probs[0][0])\n",
        "            weighted_scores.append(score * weight)\n",
        "            total_weight += weight\n",
        "\n",
        "        # print(f\"Weighted scores: {weighted_scores}\")\n",
        "        # print(f\"Total weight: {total_weight}\")\n",
        "\n",
        "        consensus_score = sum(weighted_scores) / total_weight\n",
        "        results[label] = consensus_score\n",
        "\n",
        "        # print(f\"Consensus: {consensus_score:.3f}\")\n",
        "\n",
        "\n",
        "\n",
        "    thresholds = {\"text\": 0.5, \"table\": 0.5, \"schema\": 0.5}\n",
        "    predictions = {label: score > thresholds[label] for label, score in results.items()}\n",
        "\n",
        "    return {\n",
        "        \"model\": model_name,\n",
        "        \"predictions\": predictions,\n",
        "        \"scores\": results,\n",
        "        \"thresholds\": thresholds\n",
        "    }"
      ],
      "metadata": {
        "id": "HiZNU1PDcyLu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_clip_schema_v5 = evaluate_binary_multilabel_model_v2(\n",
        "    path_folder_test_baseline\n",
        "    ,ground_truth_pu_p01_pp01_multilabel\n",
        "    ,detect_multilabel_clip_schema_v5\n",
        "    ,processor_clip\n",
        "    ,model_clip\n",
        ")\n",
        "\n",
        "print(f\"\\n📊 Résultats CLIP:\")\n",
        "print(f\"F1 Text: {metrics_clip_schema_v5['f1_text']:.3f}\")\n",
        "print(f\"F1 Table: {metrics_clip_schema_v5['f1_table']:.3f}\")\n",
        "print(f\"F1 Schema: {metrics_clip_schema_v5['f1_schema']:.3f}\")\n",
        "print(f\"F1 Macro: {metrics_clip_schema_v5['f1_macro']:.3f}\")\n",
        "print(f\"Hamming loss: {metrics_clip_schema_v5['hamming_loss']:.3f}\")\n",
        "print(f\"Jaccard: {metrics_clip_schema_v5['jaccard_macro']:.3f} & {metrics_clip_schema_v5['jaccard_micro']:.3f}\")\n",
        "print(f\"Jaccard samples :{metrics_clip_schema_v5['jaccard_samples']:.3f}\")\n",
        "print(f\"Jaccard per class: {metrics_clip_schema_v5['jaccard_per_class']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3uNfToDKfWlV",
        "outputId": "56fe48e4-8636-4c75-99a1-20f57a1df580"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " p1: ✅ ✅ ✅ | Text:0.878 vs Table:0.493 vs Schema:0.320\n",
            " p2: ✅ ✅ ✅ | Text:0.813 vs Table:0.072 vs Schema:0.087\n",
            " p3: ✅ ❌ ✅ | Text:0.982 vs Table:0.391 vs Schema:0.021\n",
            " p4: ✅ ✅ ✅ | Text:0.900 vs Table:0.255 vs Schema:0.004\n",
            " p5: ✅ ✅ ✅ | Text:0.945 vs Table:0.421 vs Schema:0.009\n",
            " p6: ✅ ✅ ✅ | Text:0.947 vs Table:0.089 vs Schema:0.050\n",
            " p7: ✅ ✅ ✅ | Text:0.896 vs Table:0.327 vs Schema:0.886\n",
            " p8: ✅ ❌ ✅ | Text:0.791 vs Table:0.467 vs Schema:0.014\n",
            " p9: ✅ ✅ ✅ | Text:0.940 vs Table:0.139 vs Schema:0.009\n",
            " p10: ✅ ✅ ✅ | Text:0.873 vs Table:0.037 vs Schema:0.069\n",
            " p11: ✅ ✅ ✅ | Text:0.507 vs Table:0.011 vs Schema:0.699\n",
            " p12: ✅ ✅ ✅ | Text:0.937 vs Table:0.818 vs Schema:0.017\n",
            " p13: ✅ ✅ ✅ | Text:0.968 vs Table:0.515 vs Schema:0.078\n",
            " p14: ✅ ✅ ✅ | Text:0.981 vs Table:0.300 vs Schema:0.026\n",
            " p15: ✅ ✅ ✅ | Text:0.963 vs Table:0.195 vs Schema:0.647\n",
            " p16: ✅ ✅ ✅ | Text:0.969 vs Table:0.140 vs Schema:0.652\n",
            " p17: ✅ ✅ ✅ | Text:0.876 vs Table:0.080 vs Schema:0.008\n",
            " p18: ✅ ✅ ✅ | Text:0.833 vs Table:0.044 vs Schema:0.001\n",
            " p19: ✅ ✅ ✅ | Text:0.829 vs Table:0.104 vs Schema:0.009\n",
            " p20: ✅ ✅ ✅ | Text:0.970 vs Table:0.407 vs Schema:0.225\n",
            " p21: ✅ ✅ ✅ | Text:0.873 vs Table:0.291 vs Schema:0.004\n",
            " p22: ✅ ✅ ✅ | Text:0.751 vs Table:0.512 vs Schema:0.005\n",
            " p23: ✅ ✅ ✅ | Text:0.832 vs Table:0.984 vs Schema:0.027\n",
            " p24: ✅ ✅ ✅ | Text:0.884 vs Table:0.431 vs Schema:0.025\n",
            " p25: ✅ ✅ ✅ | Text:0.957 vs Table:0.300 vs Schema:0.074\n",
            " p26: ✅ ✅ ✅ | Text:0.929 vs Table:0.384 vs Schema:0.080\n",
            " p27: ✅ ✅ ✅ | Text:0.951 vs Table:0.356 vs Schema:0.025\n",
            " p28: ✅ ✅ ✅ | Text:0.878 vs Table:0.366 vs Schema:0.007\n",
            " p29: ✅ ✅ ✅ | Text:0.945 vs Table:0.042 vs Schema:0.617\n",
            " p30: ✅ ✅ ✅ | Text:0.836 vs Table:0.517 vs Schema:0.026\n",
            " p31: ✅ ✅ ✅ | Text:0.943 vs Table:0.511 vs Schema:0.071\n",
            " p32: ✅ ❌ ✅ | Text:0.967 vs Table:0.068 vs Schema:0.020\n",
            " p33: ✅ ✅ ✅ | Text:0.919 vs Table:0.229 vs Schema:0.022\n",
            " p34: ✅ ✅ ✅ | Text:0.784 vs Table:0.057 vs Schema:0.010\n",
            " p35: ✅ ✅ ✅ | Text:0.906 vs Table:0.539 vs Schema:0.046\n",
            " p36: ✅ ✅ ✅ | Text:0.919 vs Table:0.565 vs Schema:0.056\n",
            " p37: ✅ ✅ ✅ | Text:0.881 vs Table:0.675 vs Schema:0.047\n",
            " p38: ✅ ❌ ✅ | Text:0.977 vs Table:0.413 vs Schema:0.021\n",
            " p39: ✅ ✅ ✅ | Text:0.888 vs Table:0.080 vs Schema:0.039\n",
            " p40: ✅ ✅ ✅ | Text:0.920 vs Table:0.197 vs Schema:0.005\n",
            " p41: ✅ ✅ ✅ | Text:0.981 vs Table:0.787 vs Schema:0.032\n",
            " p42: ✅ ✅ ✅ | Text:0.966 vs Table:0.852 vs Schema:0.023\n",
            " p43: ✅ ✅ ✅ | Text:0.975 vs Table:0.959 vs Schema:0.010\n",
            " p44: ✅ ✅ ✅ | Text:0.985 vs Table:0.901 vs Schema:0.011\n",
            " p45: ✅ ✅ ✅ | Text:0.992 vs Table:0.927 vs Schema:0.017\n",
            " p46: ✅ ✅ ✅ | Text:0.961 vs Table:0.248 vs Schema:0.051\n",
            " p47: ✅ ✅ ✅ | Text:0.989 vs Table:0.858 vs Schema:0.322\n",
            " p48: ✅ ✅ ❌ | Text:0.992 vs Table:0.949 vs Schema:0.586\n",
            " p49: ✅ ✅ ✅ | Text:0.967 vs Table:0.858 vs Schema:0.060\n",
            "\n",
            "📊 Résultats CLIP:\n",
            "F1 Text: 1.000\n",
            "F1 Table: 0.895\n",
            "F1 Schema: 0.909\n",
            "F1 Macro: 0.935\n",
            "Hamming loss: 0.034\n",
            "Jaccard: 0.881 & 0.934\n",
            "Jaccard samples :0.952\n",
            "Jaccard per class: [1.         0.80952381 0.83333333]\n"
          ]
        }
      ]
    }
  ]
}