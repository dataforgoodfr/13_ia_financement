import requests
import json
from pathlib import Path
import os
from typing import Dict, List, Any
from docx import Document
import fitz  # PyMuPDF
import logging
from datetime import datetime
import time
import sys
import re

# Constants
OLLAMA_HOST = "http://localhost:11434/api/generate"
MAX_RETRIES = 2
REQUEST_TIMEOUT = 300
CHUNK_SIZE = 300
PAUSE_BETWEEN_CHUNKS = 2

class AAPAnalyzer:
    def __init__(self, data_dir: str):
        self.data_dir = data_dir
        self.logger = self.setup_logging()
        self.output_dir = "resultats"
        os.makedirs(self.output_dir, exist_ok=True)

    def setup_logging(self) -> logging.Logger:
        """Configure et retourne le logger"""
        log_dir = "logs"
        os.makedirs(log_dir, exist_ok=True)
        
        log_file = os.path.join(log_dir, f"aap_analyzer_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log")
        
        logger = logging.getLogger('AAPAnalyzer')
        logger.setLevel(logging.INFO)
        logger.handlers = []
        
        file_handler = logging.FileHandler(log_file, encoding='utf-8')
        console_handler = logging.StreamHandler(sys.stdout)
        
        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
        file_handler.setFormatter(formatter)
        console_handler.setFormatter(formatter)
        
        logger.addHandler(file_handler)
        logger.addHandler(console_handler)
        
        return logger

    def ask_mixtral(self, prompt: str, retry_count: int = 0) -> str:
        """Version simplifi√©e qui retourne directement le texte d'analyse"""
        if retry_count >= MAX_RETRIES:
            return "Analyse non disponible (nombre maximum de tentatives atteint)"

        try:
            self.logger.info(f"Tentative {retry_count + 1}/{MAX_RETRIES}")
            
            analysis_prompt = f"""Analyse ce texte et fournis une synth√®se claire avec les points suivants :

            BESOINS PRINCIPAUX :
            - Liste des besoins identifi√©s

            ATTENTES EXPRIM√âES :
            - Liste des attentes principales

            QUESTIONS IMPORTANTES :
            - Liste des questions cl√©s

            AXES MAJEURS :
            - Points principaux du document

            CONTEXTE :
            - √âl√©ments de contexte importants

            Texte √† analyser : {prompt}"""

            response = requests.post(
                OLLAMA_HOST,
                json={
                    "model": "mixtral",
                    "prompt": analysis_prompt,
                    "stream": False,
                    "temperature": 0.1,
                },
                timeout=REQUEST_TIMEOUT
            )
            
            if response.status_code == 200:
                response_json = response.json()
                return response_json.get("response", "")
            
            time.sleep(PAUSE_BETWEEN_CHUNKS)
            return self.ask_mixtral(prompt, retry_count + 1)
            
        except Exception as e:
            self.logger.error(f"Erreur: {str(e)}")
            time.sleep(PAUSE_BETWEEN_CHUNKS)
            return self.ask_mixtral(prompt, retry_count + 1)

    def extract_text(self, file_path: str) -> str:
        """Extraction de texte des documents"""
        try:
            if file_path.lower().endswith('.docx'):
                doc = Document(file_path)
                return "\n".join([para.text for para in doc.paragraphs if para.text.strip()])
            elif file_path.lower().endswith('.pdf'):
                text = []
                with fitz.open(file_path) as doc:
                    for page in doc:
                        text.append(page.get_text())
                return "\n".join(text)
            else:
                self.logger.warning(f"Format non support√©: {file_path}")
                return ""
        except Exception as e:
            self.logger.error(f"Erreur extraction texte: {str(e)}")
            return ""

    def split_text(self, text: str, max_size: int = CHUNK_SIZE) -> List[str]:
        """D√©coupage du texte en sections"""
        if not text:
            return []
        
        # D√©coupage en paragraphes puis en chunks de taille raisonnable
        paragraphs = text.split('\n\n')
        chunks = []
        current_chunk = []
        current_size = 0
        
        for para in paragraphs:
            para_size = len(para)
            if current_size + para_size > max_size and current_chunk:
                chunks.append('\n\n'.join(current_chunk))
                current_chunk = [para]
                current_size = para_size
            else:
                current_chunk.append(para)
                current_size += para_size
        
        if current_chunk:
            chunks.append('\n\n'.join(current_chunk))
        
        return chunks
    def extract_context_around_keywords(self, text: str) -> Dict[str, List[str]]:
        """Extrait les phrases contenant les mots-cl√©s et leur contexte"""
        keywords = {
            'besoins': ['besoin', 'n√©cessite', 'requiert', 'demande'],
            'attentes': ['attente', 'attend', 'souhaite', 'd√©sire', 'exige'],
            'questions': ['question', '?', 'comment', 'pourquoi', 'quand', 'o√π', 'qui', 'quel']
        }
        
        # D√©couper le texte en phrases
        sentences = re.split(r'(?<=[.!?])\s+', text)
        context = {k: [] for k in keywords.keys()}
        
        for i, sentence in enumerate(sentences):
            for category, words in keywords.items():
                if any(word.lower() in sentence.lower() for word in words):
                    # R√©cup√©rer la phrase pr√©c√©dente si elle existe
                    previous = sentences[i-1] if i > 0 else ""
                    # R√©cup√©rer la phrase suivante si elle existe
                    next_sentence = sentences[i+1] if i < len(sentences)-1 else ""
                    
                    context_entry = {
                        'phrase': sentence.strip(),
                        'contexte_avant': previous.strip(),
                        'contexte_apres': next_sentence.strip()
                    }
                    context[category].append(context_entry)
        
        return context
    def detect_theme(self, text: str) -> str:
        """D√©tecte le th√®me principal du document"""
        try:
            theme_prompt = f"""Analyse ce texte et identifie son th√®me principal en une phrase courte.
            Ne garde que l'essentiel, maximum 10 mots.
            
            Texte √† analyser : {text[:1000]}"""  # On utilise le d√©but du document

            response = requests.post(
                OLLAMA_HOST,
                json={
                    "model": "mixtral",
                    "prompt": theme_prompt,
                    "stream": False,
                    "temperature": 0.1,
                },
                timeout=REQUEST_TIMEOUT
            )
            
            if response.status_code == 200:
                theme = response.json().get("response", "").strip()
                return theme
            
            return "Th√®me non d√©tect√©"
            
        except Exception as e:
            self.logger.error(f"Erreur d√©tection th√®me: {str(e)}")
            return "Erreur d√©tection th√®me"
    def analyze_document(self, file_path: Path) -> str:
        """Analyse d'un document avec sortie texte et contexte"""
        try:
            self.logger.info(f"\n=== Analyse de {file_path.name} ===")
            
            # Extraction du texte
            text = self.extract_text(str(file_path))
            if not text:
                return "Document vide ou non extractible"

            # D√©tection du th√®me
            theme = self.detect_theme(text)
            self.logger.info(f"TH√àME D√âTECT√â : {theme}")

            # Extraction du contexte autour des mots-cl√©s
            context_data = self.extract_context_around_keywords(text)

            # D√©coupage en sections
            chunks = self.split_text(text)
            self.logger.info(f"Document d√©coup√© en {len(chunks)} sections")
            
            analyses = []
            
            for i, chunk in enumerate(chunks, 1):
                self.logger.info(f"Analyse section {i}/{len(chunks)}")
                analysis = self.ask_mixtral(chunk)
                if analysis:
                    analyses.append(analysis)
                    # Log les 3 premi√®res analyses
                    if i <= 3:
                        self.logger.info(f"\nANALYSE SECTION {i}:\n{'-' * 20}\n{analysis}\n")
                time.sleep(PAUSE_BETWEEN_CHUNKS)

            # Compilation de l'analyse compl√®te avec le contexte
            full_analysis = f"""
{"=" * 50}
ANALYSE DE : {file_path.name}
{"=" * 50}

TH√àME PRINCIPAL : {theme}
{"-" * 50}

"""
            # Ajouter les analyses par section
            for i, analysis in enumerate(analyses, 1):
                full_analysis += f"\nSECTION {i} :\n{'-' * 20}\n{analysis}\n\n"

            # Ajouter les extraits avec contexte
            full_analysis += f"\n{'=' * 50}\nEXTRAITS AVEC CONTEXTE\n{'=' * 50}\n\n"
            
            for category, entries in context_data.items():
                if entries:
                    full_analysis += f"\n{category.upper()} IDENTIFI√âS :\n{'-' * 20}\n"
                    for entry in entries:
                        full_analysis += f"\nContexte avant : {entry['contexte_avant']}\n"
                        full_analysis += f"PHRASE : {entry['phrase']}\n"
                        full_analysis += f"Contexte apr√®s : {entry['contexte_apres']}\n"
                        full_analysis += f"{'-' * 40}\n"

            # Sauvegarde dans un fichier texte
            output_file = os.path.join(self.output_dir, f"analyse_{file_path.stem}.txt")
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write(full_analysis)

            return full_analysis

        except Exception as e:
            self.logger.error(f"Erreur analyse document: {str(e)}")
            return f"Erreur lors de l'analyse: {str(e)}"

def main():
    try:
        print("\nüîç Initialisation de l'analyse des AAP...")
        
        # V√©rification et cr√©ation des r√©pertoires
        data_dir = Path("../data").resolve()
        if not data_dir.exists():
            print(f"‚ùå R√©pertoire data non trouv√©: {data_dir}")
            return
            
        analyzer = AAPAnalyzer(str(data_dir))
        
        # Test de connexion Mixtral
        print("üîÑ Test de connexion √† Mixtral...")
        test_response = analyzer.ask_mixtral("Test de connexion")
        if not test_response:
            print("‚ùå Impossible de se connecter √† Mixtral")
            return
        print("‚úÖ Connexion √† Mixtral √©tablie")
        
        # Recherche des fichiers AAP
        print("\nüîç Recherche des fichiers AAP...")
        aap_files = list(data_dir.rglob("*.[pd][do][fc]*"))
        aap_files = [f for f in aap_files if f.suffix.lower() in ('.pdf', '.docx')]
        
        if not aap_files:
            print("‚ùå Aucun fichier AAP trouv√©")
            return
            
        print(f"üìÅ {len(aap_files)} fichiers AAP trouv√©s")
        
        # Analyse des documents
        for i, file_path in enumerate(aap_files, 1):
            print(f"\nüìÑ [{i}/{len(aap_files)}] Analyse de {file_path.name}")
            analysis = analyzer.analyze_document(file_path)
            
            # Affichage d'un r√©sum√©
            print("\nüìù R√©sum√© de l'analyse :")
            print("-" * 50)
            theme_match = re.search(r"TH√àME PRINCIPAL : (.+?)\n", analysis)
            if theme_match:
                print(f"üè∑Ô∏è Th√®me : {theme_match.group(1)}")
            print("-" * 50)
            # Affiche les 500 premiers caract√®res avec "..." si le texte est plus long
            print(analysis[:500] + "..." if len(analysis) > 500 else analysis)
            print("-" * 50)
            print(f"üíæ Analyse compl√®te sauvegard√©e dans resultats/analyse_{file_path.stem}.txt")
            
        print("\n‚úÖ Analyse termin√©e !")
        print(f"üìä {len(aap_files)} documents analys√©s")
        print("üíæ Les r√©sultats complets sont disponibles dans le dossier 'resultats'")
            
    except Exception as e:
        print(f"\n‚ùå Erreur: {str(e)}")

if __name__ == "__main__":
    main()